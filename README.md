# AI4Med
| No. | Name |Imaging |   Detail| Link |
|1   :|BraTS| MRI|------|[üîó](http://braintumorsegmentation.org/)|
|2   :|CIRS| B-mode|------|[üîó](https://service.tib.eu/ldmservice/dataset/cirs-phantom-dataset)|


| No. | Name | Paper Title | Pub |Imaging | Based | Main Contribution |  Abstract | Code |
|----:|--------|-------------------|------|------|------|---------------------------------|-----------|----------|
| 1   | [SLATER](https://arxiv.org/pdf/2105.08059) | Unsupervised MRI Reconstruction via Zero-Shot Learned Adversarial Transformers | TMI(2022) | MRI| Volume| Unsupervised MRI Reconstruction via Zero-Shot Learned Adversarial Transformers  | <details><summary>Click</summary> Supervised reconstruction models are characteristically trained on matched pairs of undersampled and fully-sampled data to capture an MRI prior, along with supervision regarding the imaging operator to enforce data consistency. To reduce supervision requirements, the recent deep image prior framework instead conjoins untrained MRI priors with the imaging operator during inference. Yet, canonical convolutional architectures are suboptimal in capturing long-range relationships, and priors based on randomly initialized networks may yield suboptimal performance. To address these limitations, here we introduce a novel unsupervised MRI reconstruction method based on zero-Shot Learned Adversarial TransformERs (SLATER). SLATER embodies a deep adversarial network with cross-attention transformers to map noise and latent variables onto coil-combined MR images. During pre-training, this unconditional network learns a high-quality MRI prior in an unsupervised generative modeling task. During inference, a zero-shot reconstruction is then performed by incorporating the imaging operator and optimizing the prior to maximize consistency to undersampled data. Comprehensive experiments on brain MRI datasets clearly demonstrate the superior performance of SLATER against state-of-the-art unsupervised methods. </details> | [üîó](https://github.com/icon-lab/SLATER)
| 2   | [DEER](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9239986) | Deep Efficient End-to-End Reconstruction(DEER) Network for Few-View Breast CT Image Reconstruction | IEEE Access(2020) | CT | Volume | Deep efficient end-to-end reconstruction (DEER)network for few-view breast CT image reconstruction   | <details><summary>Click</summary> Breast CT provides image volumes with isotropic resolution in high contrast, enabling detection of small calcification (down to a few hundred microns in size) and subtle density differences. Since breast is sensitive to x-ray radiation, dose reduction of breast CT is an important topic, and for this purpose, few-view scanning is a main approach. In this article, we propose a Deep Efficient End-to-end Reconstruction (DEER) network for few-view breast CT image reconstruction. The major merits of our network include high dose efficiency, excellent image quality, and low model complexity. By the design, the proposed network can learn the reconstruction process with as few as (9(N) parameters, where N is the side length of an image to be reconstructed, which represents orders of magnitude improvements relative to the state-of-the-art deeplearning-based reconstruction methods that map raw data to tomographic images directly. Also, validated on a cone-beam breast CT dataset prepared by Koning Corporation on a commercial scanner, our method demonstrates a competitive performance over the state-of-the-art reconstruction networks in terms of image quality. The source code of this paper is available at: https://github.com/HuidongXie/DEER. </details> | [üîó](https://github.com/HuidongXie/DEER)
| 3   | [C¬≤RV](https://openaccess.thecvf.com/content/CVPR2024/papers/Lin_C2RV_Cross-Regional_and_Cross-View_Learning_for_Sparse-View_CBCT_Reconstruction_CVPR_2024_paper.pdf) | C^ 2RV: Cross-Regional and Cross-View Learning for Sparse-View CBCT Reconstruction | CVPR(2024) | CT | Volume|Explicit multi-scale volumetric representations are utilized to enable cross - regional learning in the 3D space. The scale - view cross - attention module is introduced to adaptively aggregate multi - scale and multi - view features   | <details><summary>Click</summary> Cone beam computed tomography (CBCT) is an important imaging technology widely used in medical scenarios such as diagnosis and preoperative planning. Using fewer projection views to reconstruct CT also known as sparse-view reconstruction can reduce ionizing radiation and further benefit interventional radiology. Compared with sparse-view reconstruction for traditional parallel/fan-beam CT CBCT reconstruction is more challenging due to the increased dimensionality caused by the measurement process based on cone-shaped X-ray beams. As a 2D-to-3D reconstruction problem although implicit neural representations have been introduced to enable efficient training only local features are considered and different views are processed equally in previous works resulting in spatial inconsistency and poor performance on complicated anatomies. To this end we propose C^2RV by leveraging explicit multi-scale volumetric representations to enable cross-regional learning in the 3D space. Additionally the scale-view cross-attention module is introduced to adaptively aggregate multi-scale and multi-view features. Extensive experiments demonstrate that our C^2RV achieves consistent and significant improvement over previous state-of-the-art methods on datasets with diverse anatomy. Code is available at https://github.com/xmed-lab/C2RV-CBCT. </details> | [üîó](https://github.com/xmed-lab/C2RV-CBCT)
| 4 | [extended-MedNeRF](https://www.biorxiv.org/content/10.1101/2023.04.24.538160v1.full.pdf) | 3D reconstructions of brain from MRI scans using neural radiance fields | ICAISC (2023) |MRI | NRF | a neural radiance field-based approach for reconstructing 3D projections from 2D MRI slices | <details><summary>Click</summary>The advent of 3D Magnetic Resonance Imaging (MRI) has revolutionized medical imaging and diagnostic capabilities, allowing for more precise diagnosis, treatment planning, and improved patient outcomes. 3D MRI imaging enables the creation of detailed 3D reconstructions of anatomical structures that can be used for visualization, analysis, and surgical planning. However, these reconstructions often require many scan acquisitions, demanding a long session to use the machine and requiring the patient to remain still, with consequent possible motion artifacts. The development of neural radiance fields (NeRF) technology has shown promising results in generating highly accurate 3D reconstructions of MRI images with less user input. Our approach is based on neural radiance fields to reconstruct 3D projections from 2D slices of MRI scans. We do this by using 3D convolutional neural networks to address challenges posed by variable slice thickness; incorporating multiple MRI modalities to ensure robustness and extracting the shape and volumetric depth of both surface and internal anatomical structures with slice interpolation. This approach provides more comprehensive and robust 3D reconstructions of both surface and internal anatomical structures and has significant potential for clinical applications, allowing medical professionals to better visualize and analyze anatomical structures with less available data, potentially reducing times and motion-related issues. </details>  | ‚ùå
| 5 | [NAF](https://arxiv.org/pdf/2209.14540) | Neural Attenuation Fields for Sparse-View  CBCT Reconstruction | MICCAI (2022) |CT  | NRF | a self-supervised solution for sparse-view CBCT reconstruction without external training data | <details><summary>Click</summary>This paper proposes a novel and fast self-supervised solution for sparse-view CBCT reconstruction (Cone Beam Computed Tomography) that requires no external training data. Specifically, the desired attenuation coefficients are represented as a continuous function of 3D spatial coordinates, parameterized by a fully-connected deep neural network. We synthesize projections discretely and train the network by minimizing the error between real and synthesized projections. A learning-based encoder entailing hash coding is adopted to help the network capture high-frequency details. This encoder outperforms the commonly used frequency-domain encoder in terms of having higher performance and efficiency, because it exploits the smoothness and sparsity of human organs. Experiments have been conducted on both human organ and phantom datasets. The proposed method achieves state-of-the-art accuracy and spends reasonably short computation time. </details>  | [üîó](https://github.com/Ruyi-Zha/naf_cbct) 
| 6 | [DIF-Net](https://arxiv.org/pdf/2303.06681) |Learning Deep Intensity Field for Extremely  Sparse-View CBCT Reconstruction| MICCAI (2023) |CT  | IPE |A supervised CBCT reconstruction framework for high-quality, high-resolution CBCT from $\leq10$ sparse views| <details><summary>Click</summary>Sparse-view cone-beam CT (CBCT) reconstruction is an important direction to reduce radiation dose and benefit clinical applications. Previous voxel-based generation methods represent the CT as discrete voxels, resulting in high memory requirements and limited spatial resolution due to the use of 3D decoders. In this paper, we formulate the CT volume as a continuous intensity field and develop a novel DIF-Net to perform high-quality CBCT reconstruction from extremely sparse (‚â§10) projection views at an ultrafast speed. The intensity field of a CT can be regarded as a continuous function of 3D spatial points. Therefore, the reconstruction can be reformulated as regressing the intensity value of an arbitrary 3D point from given sparse projections. Specifically, for a point, DIF-Net extracts its view-specific features from different 2D projection views. These features are subsequently aggregated by a fusion module for intensity estimation. Notably, thousands of points can be processed in parallel to improve efficiency during training and testing. In practice, we collect a knee CBCT dataset to train and evaluate DIF-Net. Extensive experiments show that our approach can reconstruct CBCT with high image quality and high spatial resolution from extremely sparse views within 1.6¬†s, significantly outperforming state-of-the-art methods. Our code will be available at https://github.com/xmed-lab/DIF-Net. </details>  | [üîó](https://github.com/xmed-lab/DIF-Net) 
| 7 | [Ultra-NeRF](https://proceedings.mlr.press/v227/wysocki24a/wysocki24a.pdf) |Ultra-NeRF: Neural Radiance Fields for Ultrasound Imaging |PMLRÔºà2024Ôºâ |3D US  | NRF | AA ray-tracing-based method for synthesizing accurate B-mode images by learning view-dependent scene appearance and geometry from multiple US sweeps| <details><summary>Click</summary>We present a physics-enhanced implicit neural representation (INR) for ultrasound (US) imaging that learns tissue properties from overlapping US sweeps. Our proposed method leverages a ray-tracing-based neural rendering for novel view US synthesis. Recent publications demonstrated that INR models could encode a representation of a three-dimensional scene from a set of two-dimensional US frames. However, these models fail to consider the view-dependent changes in appearance and geometry intrinsic to US imaging. In our work, we discuss direction-dependent changes in the scene and show that a physics-inspired rendering improves the fidelity of US image synthesis. In particular, we demonstrate experimentally that our proposed method generates geometrically accurate B-mode images for regions with ambiguous representation owing to view-dependent differences of the US images. We conduct our experiments using simulated B-mode US sweeps of the liver and acquired US sweeps of a spine phantom tracked with a robotic arm. The experiments corroborate that our method generates US frames that enable consistent volume compounding from previously unseen views. To the best of our knowledge, the presented work is the first to address view-dependent US image synthesis using INR.</details>  | ‚ùå
| 8 | [RepMedGraf](https://neural-fields-beyond-cams.github.io/accepted_papers/22.pdf) |RepMedGraf: Re-parameterization Medical Generated Radiation Field for Improved 3D Image Reconstruction |AVSS(2024) |CT | NRF | A lightweight RepVGG-based model for solving overfitting and improving training efficiency| <details><summary>Click</summary>In the field of medical imaging, 3D image reconstruction has emerged as a crucial technique for accurate disease diagnosis. Neural Radiance Field (NeRF) has shown promise in generating high-quality 3D models through deep learning, but its application in medical imaging remains limited. This paper presents RepMedGraf, an improved model addressing the limitations of NeRF. RepMedGraf utilizes a generator based on lightweight RepVGG Blocks instead of MedNeRF model. By doing so, the risk of overfitting is reduced, and training efficiency is improved. The proposed method is trained on publicly available chest and knee datasets. Comparative evaluations are conducted based on the generated radiation fields, demonstrating the effectiveness and quality of the 3D models produced by RepMedGraf. The results highlight the potential of RepMedGraf as a valuable tool in medical imaging for enhanced diagnostic accuracy and improved patient care. </details>  | ‚ùå
| 9 | [ACnerf](https://iopscience.iop.org/article/10.1088/1361-6560/ad1d6c/meta) |ACnerf: enhancement of neural radiance field by alignment and correction of pose to reconstruct new views from a single x-ray* |IPEM(2024) |CT | NRF | A GRAF-based method to enhance the generalization of NeRF and improve the quality of generated images through alignment and pose correction| <details><summary>Click</summary>Objective. Computed tomography (CT) is widely used in medical research and clinical diagnosis. However, acquiring CT data requires patients to be exposed to considerable ionizing radiance, leading to physical harm. Recent studies have considered using neural radiance field (NERF) techniques to infer the full-view CT projections from single-view x-ray projection, thus aiding physician judgment and reducing Radiance hazards. This paper enhances this technique in two directions: (1) accurate generalization capabilities for control models. (2) Consider different ranges of viewpoints. Approach. Building upon generative radiance fields (GRAF), we propose a method called ACnerf to enhance the generalization of the NERF through alignment and pose correction. ACnerf aligns with a reference single x-ray by utilizing a combination of positional encoding with Gaussian random noise (latent code) obtained from GRAF training. This approach avoids compromising the 3D structure caused by altering the generator. During inference, a pose judgment network is employed to correct the pose and optimize the rendered viewpoint. Additionally, when generating a narrow range of views, ACnerf employs frequency-domain regularization to fine-tune the generator and achieve precise projections. Main results. The proposed ACnerf method surpasses the state-of-the-art NERF technique in terms of rendering quality for knee and chest data with varying contrasts. It achieved an average improvement of 2.496 dB in PSNR and 41% in LPIPS for 0¬∞‚Äì360¬∞ projections. Additionally, for ‚àí15¬∞ to 15¬∞ projections, ACnerf achieved an average improvement of 0.691 dB in PSNR and 25.8% in LPIPS. Significance. With adjustments in alignment, inference, and rendering range, our experiments and evaluations on knee and chest data of different contrasts show that ACnerf effectively reduces artifacts and aberrations in the new view. ACnerf's ability to recover more accurate 3D structures from single x-rays has excellent potential for reducing damage from ionising radiation in clinical diagnostics.</details>  | ‚ùå
| 10 | [Oral-3Dv2](https://arxiv.org/pdf/2303.12123) |Oral-3Dv2: 3D Oral Reconstruction from Panoramic X-Ray Imaging with Implicit Neural Representation |arXiv(2023) |CT | NRF | A multi-head, dynamic-sampling, view-independent NeXF model for 3D oral reconstruction from panoramic X-ray imaging with implicit neural representation| <details><summary>Click</summary>3D reconstruction of medical imaging from 2D images has become an increasingly interesting topic with the development of deep learning models in recent years. Previous studies in 3D reconstruction from limited X-ray images mainly rely on learning from paired 2D and 3D images, where the reconstruction quality relies on the scale and variation of collected data. This has brought significant challenges in the collection of training data, as only a tiny fraction of patients take two types of radiation examinations in the same period. Although simulation from higher-dimension images could solve this problem, the variance between real and simulated data could bring great uncertainty at the same time. In oral reconstruction, the situation becomes more challenging as only a single panoramic X-ray image is available, where models need to infer the curved shape by prior individual knowledge. To overcome these limitations, we propose Oral-3Dv2 to solve this cross-dimension translation problem in dental healthcare by learning solely on projection information, i.e., the projection image and trajectory of the X-ray tube. Our model learns to represent the 3D oral structure in an implicit way by mapping 2D coordinates into density values of voxels in the 3D space. To improve efficiency and effectiveness, we utilize a multi-head model that predicts a bunch of voxel values in 3D space simultaneously from a 2D coordinate in the axial plane and the dynamic sampling strategy to refine details of the density distribution in the reconstruction result. Extensive experiments in simulated and real data show that our model significantly outperforms existing state-of-the-art models without learning from paired images or prior individual knowledge. To the best of our knowledge, this is the first work of a non-adversarial-learning-based model in 3D radiology reconstruction from a single panoramic X-ray image.</details>  |  ‚ùå
| 11 | [PINER](https://openaccess.thecvf.com/content/WACV2023/papers/Song_PINER_Prior-Informed_Implicit_Neural_Representation_Learning_for_Test-Time_Adaptation_in_WACV_2023_paper.pdf) |PINER: Prior-informed Implicit Neural Representation Learning for Test-time  Adaptation in Sparse-view CT Reconstruction |WACV (2023) |CT | IPE | A novel two-stage source-free black-box testtime adaptation algorithm for sparse-view CT reconstruction with unknown noise through prior-informed implicit neural representation learning| <details><summary>Click</summary>Recently, deep learning has been introduced to solve important medical image reconstruction problems such as sparse-view CT reconstruction. However, the developed deep reconstruction models are generally limited in generalization when applied to unseen testing samples in target domain. Furthermore, privacy concerns may impede the availability of source-domain training data to retrain or adapt the model to the target-domain testing data, which are quite common in real-world medical applications. To address these issues, we introduce a source-free black-box test-time adaptation method for sparse-view CT reconstruction with unknown noise levels based on prior-informed implicit neural representation learning (PINER). By leveraging implicit neural representation learning to generate the image representations at various noise levels, the proposed method is able to construct the adapted input representations at test time based on the inference of black-box model and output analysis. We performed experiments of source-free test-time adaptation for sparse-view CT reconstruction with unknown noise levels on multiple anatomical sites with different black-box deep reconstruction models, where our method outperforms the state-of-the-art algorithms. </details>  |   [üîó](https://github.com/efzero/PINER) 
| 12 | [VolumeNeRF](https://papers.miccai.org/miccai-2024/paper/3061_paper.pdf) |VolumeNeRF: CT Volume Reconstruction from a Single Projection View |MICCAI (2024) |CT | NRF | A NeRF-based model for 3D CT reconstruction from a single-view X-ray image| <details><summary>Click</summary>Computed tomography (CT) plays a significant role in clinical practice by providing detailed three-dimensional information, aiding in accurate assessment of various diseases. However, CT imaging requires a large number of X-ray projections from different angles and exposes patients to high doses of radiation. Here we propose VolumeNeRF, based on neural radiance fields (NeRF), for reconstructing CT volumes from a single-view X-ray. During training, our network learns to generate a continuous representation of the CT scan conditioned on the input X-ray image and render an X-ray image similar to the input from the same viewpoint as the input. Considering the ill-posedness and the complexity of the single-perspective generation task, we introduce likelihood images and the average CT images to incorporate prior anatomical knowledge. A novel projection attention module is designed to help the model learn the spatial correspondence between voxels in CT images and pixels in X-ray images during the imaging process. Extensive experiments conducted on a publicly available chest CT dataset show that our VolumeNeRF achieves better performance than other state-of-the-art methods. Our code is available at https://www.github.com/Aurora132/VolumeNeRF. </details>  |   [üîó](https://www.github.com/Aurora132/VolumeNeRF) 
| 13 | [NeSVoR](https://www.researchgate.net/profile/Junshen-Xu/publication/365043351_NeSVoR_Implicit_Neural_Representation_for_Slice-to-Volume_Reconstruction_in_MRI/links/636ab7c5431b1f53007e10d9/NeSVoR-Implicit-Neural-Representation-for-Slice-to-Volume-Reconstruction-in-MRI.pdf) |NeSVoR: Implicit Neural Representation for Slice-to-Volume Reconstruction in MRI | TMI (2023) |CT MRI | NRF | A resolutionagnostic slice-to-volume reconstruction method, which models the underlying volume as a continuous function of spatial coordinates with implicit neural representation| <details><summary>Click</summary>Reconstructing 3D MR volumes from multiple motion-corrupted stacks of 2D slices has shown promise in imaging of moving subjects, e. g., fetal MRI. However, existing slice-to-volume reconstruction methods are time-consuming, especially when a high-resolution volume is desired. Moreover, they are still vulnerable to severe subject motion and when image artifacts are present in acquired slices. In this work, we present NeSVoR, a resolution-agnostic slice-to-volume reconstruction method, which models the underlying volume as a continuous function of spatial coordinates with implicit neural representation. To improve robustness to subject motion and other image artifacts, we adopt a continuous and comprehensive slice acquisition model that takes into account rigid inter-slice motion, point spread function, and bias fields. NeSVoR also estimates pixel-wise and slice-wise variances of image noise and enables removal of outliers during reconstruction and visualization of uncertainty. Extensive experiments are performed on both simulated and in vivo data to evaluate the proposed method. Results show that NeSVoR achieves state-of-the-art reconstruction quality while providing two to ten-fold acceleration in reconstruction times over the state-of-the-art algorithms. </details>  |   [üîó](https://github.com/daviddmc/NeSVoR) 
| 14 | [UMedNeRF](https://xplore..org/abstract/document/10635864) |UMedNeRF: Uncertainty-Aware Single View Volumetric Rendering For Medical Neural Radiance Fields|ISBI(2024) |CT | NRF | A radiance field-based network for continuous CT projection representation from 2D X-rays, with internal structure extraction and multi-task loss optimization| <details><summary>Click</summary>In the field of clinical medicine, computed tomography (CT) is an effective medical imaging modality for the diagnosis of various pathologies. Compared with X-ray images, CT images can provide more information, including multi-planar slices and three-dimensional structures for clinical diagnosis. However, CT imaging requires patients to be exposed to large doses of ionizing radiation for a long time, which may cause irreversible physical harm. In this paper, we propose an Uncertainty-aware MedNeRF (UMedNeRF) network based on generated radiation fields. This network can learn a continuous representation of CT projections from 2D X-ray images by obtaining the internal structure and depth information and using multi-task adaptive loss weights to ensure the quality of the generated images. Our model is trained on publicly available knee and chest datasets, and we show the results of CT projection rendering with a single X-ray and compare our method with other methods based on generated radiation fields. </details>  | ‚ùå
| 15 | [UlRe-NeRF](https://arxiv.org/pdf/2408.00860) |UlRe-NeRF: 3D Ultrasound Imaging through  Neural Rendering with Ultrasound Reflection  Direction Parameterization|	arXiv(2024) |3D US | NRF | A ultrasound neural rendering model integrating implicit neural networks and explicit volume rendering, featuring reflection direction parameterization and harmonic encoding| <details><summary>Click</summary>Three-dimensional ultrasound imaging is a critical technology widely used in medical diagnostics. However, traditional 3D ultrasound imaging methods have limitations such as fixed resolution, low storage efficiency, and insufficient contextual connectivity, leading to poor performance in handling complex artifacts and reflection characteristics. Recently, techniques based on NeRF (Neural Radiance Fields) have made significant progress in view synthesis and 3D reconstruction, but there remains a research gap in high-quality ultrasound imaging. To address these issues, we propose a new model, UlRe-NeRF, which combines implicit neural networks and explicit ultrasound volume rendering into an ultrasound neural rendering architecture. This model incorporates reflection direction parameterization and harmonic encoding, using a directional MLP module to generate view-dependent high-frequency reflection intensity estimates, and a spatial MLP module to produce the medium's physical property parameters. These parameters are used in the volume rendering process to accurately reproduce the propagation and reflection behavior of ultrasound waves in the medium. Experimental results demonstrate that the UlRe-NeRF model significantly enhances the realism and accuracy of high-fidelity ultrasound image reconstruction, especially in handling complex medium structures. </details>  |  ‚ùå
| 16 | [ImplicitVol](https://arxiv.org/pdf/2109.12108) |ImplicitVol: Sensorless 3D Ultrasound Reconstruction with Deep Implicit Representation|	arXiv(2021) |3D US | NRF | A model for deep implicit 3D volume sensorless  reconstruction from 2D freehand ultrasound images, featuring implicit spatial-to-intensity mapping| <details><summary>Click</summary>The objective of this work is to achieve sensorless reconstruction of a 3D volume from a set of 2D freehand ultrasound images with deep implicit representation. In contrast to the conventional way that represents a 3D volume as a discrete voxel grid, we do so by parameterizing it as the zero level-set of a continuous function, i.e. implicitly representing the 3D volume as a mapping from the spatial coordinates to the corresponding intensity values. Our proposed model, termed as ImplicitVol, takes a set of 2D scans and their estimated locations in 3D as input, jointly refining the estimated 3D locations and learning a full reconstruction of the 3D volume. When testing on real 2D ultrasound images, novel cross-sectional views that are sampled from ImplicitVol show significantly better visual quality than those sampled from existing reconstruction approaches, outperforming them by over 30% (NCC and SSIM), between the output and ground-truth on the 3D volume testing data. The code will be made publicly available. </details>  | ‚ùå
| 17 | [FUNSR](https://www.sciencedirect.com/science/article/abs/pii/S1361841524002305) |Neural implicit surface reconstruction of freehand 3D ultrasound volume with geometric constraints|	MIA(2024) |3D US | NRF | A self-supervised neural implicit surface reconstruction with two geometric constraints method to learn signed distance functions (SDFs) from US volumes| <details><summary>Click</summary>Three-dimensional (3D) freehand ultrasound (US) is a widely used imaging modality that allows non-invasive imaging of medical anatomy without radiation exposure. Surface reconstruction of US volume is vital to acquire the accurate anatomical structures needed for modeling, registration, and visualization. However, traditional methods cannot produce a high-quality surface due to image noise. Despite improvements in smoothness, continuity, and resolution from deep learning approaches, research on surface reconstruction in freehand 3D US is still limited. This study introduces FUNSR, a self-supervised neural implicit surface reconstruction method to learn signed distance functions (SDFs) from US volumes. In particular, FUNSR iteratively learns the SDFs by moving the 3D queries sampled around volumetric point clouds to approximate the surface, guided by two novel geometric constraints: sign consistency constraint and on-surface constraint with adversarial learning. Our approach has been thoroughly evaluated across four datasets to demonstrate its adaptability to various anatomical structures, including a hip phantom dataset, two vascular datasets and one publicly available prostate dataset. We also show that smooth and continuous representations greatly enhance the visual appearance of US data. Furthermore, we highlight the potential of our method to improve segmentation performance, and its robustness to noise distribution and motion perturbation. </details>  |  [üîó](https://github.com/chenhbo/FUNSR) 
| 18 | [GONG et al.](https://www.researchgate.net/profile/Kuang-Gong/publication/352643894_Direct_Reconstruction_of_Linear_Parametric_Images_from_Dynamic_PET_Using_Nonlocal_Deep_Image_Prior/links/60d4900f299bf1fe469b2deb/Direct-Reconstruction-of-Linear-Parametric-Images-from-Dynamic-PET-Using-Nonlocal-Deep-Image-Prior.pdf) |Direct Reconstruction of Linear Parametric Images From Dynamic PET Using Nonlocal Deep Image Prior|	TMI(2022) |PET | IPE | An unsupervised deep learning framework inspired by the DIP framework and the nonlocal concept for direct parametric reconstruction from dynamic PET| <details><summary>Click</summary>Direct reconstruction methods have been developed to estimate parametric images directly from the measured PET sinograms by combining the PET imaging model and tracer kinetics in an integrated framework. Due to limited counts received, signal-to-noise-ratio (SNR) and resolution of parametric images produced by direct reconstruction frameworks are still limited. Recently supervised deep learning methods have been successfully applied to medical imaging denoising/reconstruction when large number of high-quality training labels are available. For static PET imaging, high-quality training labels can be acquired by extending the scanning time. However, this is not feasible for dynamic PET imaging, where the scanning time is already long enough. In this work, we proposed an unsupervised deep learning framework for direct parametric reconstruction from dynamic PET, which was tested on the Patlak model and the relative equilibrium Logan model. The training objective function was based on the PET statistical model. The patient‚Äôs anatomical prior image, which is readily available from PET/CT or PET/MR scans, was supplied as the network input to provide a manifold constraint, and also utilized to construct a kernel layer to perform non-local feature denoising. The linear kinetic model was embedded in the network structure as a 1√ó1√ó1 convolution layer. Evaluations based on dynamic datasets of 18F-FDG and 11C-PiB tracers show that the proposed framework can outperform the traditional and the kernel method-based direct reconstruction methods.</details>  | ‚ùå
| 29 | [NeRF-US](https://arxiv.org/pdf/2408.10258) |NeRF-US: Removing Ultrasound Imaging Artifacts from Neural Radiance Fields in the Wild|		arXiv(2024) |3D US | NRF | A approach for training NeRFs on ultrasound imaging that incorporates the properties of ultrasound imaging and incorporates 3D priors through a diffusion model , also utilizing ultrasound-specific rendering| <details><summary>Click</summary>Current methods for performing 3D reconstruction and novel view synthesis (NVS) in ultrasound imaging data often face severe artifacts when training NeRF-based approaches. The artifacts produced by current approaches differ from NeRF floaters in general scenes because of the unique nature of ultrasound capture. Furthermore, existing models fail to produce reasonable 3D reconstructions when ultrasound data is captured or obtained casually in uncontrolled environments, which is common in clinical settings. Consequently, existing reconstruction and NVS methods struggle to handle ultrasound motion, fail to capture intricate details, and cannot model transparent and reflective surfaces. In this work, we introduced NeRF-US, which incorporates 3D-geometry guidance for border probability and scattering density into NeRF training, while also utilizing ultrasound-specific rendering over traditional volume rendering. These 3D priors are learned through a diffusion model. Through experiments conducted on our new "Ultrasound in the Wild" dataset, we observed accurate, clinically plausible, artifact-free reconstructions. </details>  |  [üîó](rishitdagli.com/nerf-us/) 
| 20 | [MedNeRF](https://arxiv.org/pdf/2202.01020) |MedNeRF: Medical Neural Radiance Fields for Reconstructing 3D-aware CT-Projections from a Single X-ray|		EMBC(2022) |CT | NRF | A Deep Learning model that learns to reconstruct CT projections from a few or even a single-view X-ray based on a novel architecture that builds from neural radiance fields| <details><summary>Click</summary>Computed tomography (CT) is an effective med-ical imaging modality, widely used in the field of clinical medicine for the diagnosis of various pathologies. Advances in Multidetector CT imaging technology have enabled additional functionalities, including generation of thin slice multi planar cross-sectional body imaging and 3D reconstructions. However, this involves patients being exposed to a considerable dose of ionising radiation. Excessive ionising radiation can lead to deterministic and harmful effects on the body. This paper proposes a Deep Learning model that learns to reconstruct CT projections from a few or even a single-view X-ray. This is based on a novel architecture that builds from neural radiance fields, which learns a continuous representation of CT scans by disentangling the shape and volumetric depth of surface and internal anatomical structures from 2D images. Our model is trained on chest and knee datasets, and we demonstrate qual-itative and quantitative high-fidelity renderings and compare our approach to other recent radiance field-based methods. Our code and link to our datasets are available at https://qithub.com/abrilcf/mednerf Clinical relevance- Our model is able to infer the anatomical 3D structure from a few or a single-view X-ray showing future potential for reduced ionising radiation exposure during the imaging process. </details>  |  [üîó](https://github.com/abrilcf/mednerf) 
| 21 | [CuNeRF](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_CuNeRF_Cube-Based_Neural_Radiance_Field_for_Zero-Shot_Medical_Image_Arbitrary-Scale_ICCV_2023_paper.pdf) |CuNeRF: Cube-Based Neural Radiance Field for Zero-Shot Medical Image Arbitrary-Scale Super Resolution|		ICCV(2023) |CT MRI | NRF | A zero-shot MIASSR framework builds continuous volumetric representations from LR volumes, generating medical images at any scale and viewpoint| <details><summary>Click</summary> Medical image arbitrary-scale super-resolution (MIASSR) has recently gained widespread attention, aiming to supersample medical volumes at arbitrary scales via a single model. However, existing MIASSR methods face two major limitations: (i) reliance on high-resolution (HR) volumes and (ii) limited generalization ability, which restricts their applications in various scenarios. To overcome these limitations, we propose Cube-based Neural Radiance Field (CuNeRF), a zero-shot MIASSR framework that is able to yield medical images at arbitrary scales and free viewpoints in a continuous domain. Unlike existing MISR methods that only fit the mapping between low-resolution (LR) and HR volumes, CuNeRF focuses on building a continuous volumetric representation from each LR volume without the knowledge from the corresponding HR one. This is achieved by the proposed differentiable modules: cube-based sampling, isotropic volume rendering, and cube-based hierarchical rendering. Through extensive experiments on magnetic resource imaging (MRI) and computed tomography (CT) modalities, we demonstrate that CuNeRF can synthesize high-quality SR medical images, which outperforms state-of-the-art MISR methods, achieving better visual verisimilitude and fewer objectionable artifacts. Compared to existing MISR methods, our CuNeRF is more applicable in practice.</details>  |  [üîó](NarcissusEx.github.io/CuNeRF) 
| 22 | [SAX-NeRF](https://openaccess.thecvf.com/content/ICCV2023/papers/Chen_CuNeRF_Cube-Based_Neural_Radiance_Field_for_Zero-Shot_Medical_Image_Arbitrary-Scale_ICCV_2023_paper.pdf) |Structure-Aware Sparse-View X-ray 3D Reconstruction|		CVPR(2024) |CT  | NRF | A framework for sparse-view X-ray 3D reconstruction without CT data for training | <details><summary>Click</summary> X-ray known for its ability to reveal internal structures of objects is expected to provide richer information for 3D reconstruction than visible light. Yet existing NeRF algorithms overlook this nature of X-ray leading to their limitations in capturing structural contents of imaged objects. In this paper we propose a framework Structure-Aware X-ray Neural Radiodensity Fields (SAX-NeRF) for sparse-view X-ray 3D reconstruction. Firstly we design a Line Segment-based Transformer (Lineformer) as the backbone of SAX-NeRF. Linefomer captures internal structures of objects in 3D space by modeling the dependencies within each line segment of an X-ray. Secondly we present a Masked Local-Global (MLG) ray sampling strategy to extract contextual and geometric information in 2D projection. Plus we collect a larger-scale dataset X3D covering wider X-ray applications. Experiments on X3D show that SAX-NeRF surpasses previous NeRF-based methods by 12.56 and 2.49 dB on novel view synthesis and CT reconstruction. https://github.com/caiyuanhao1998/SAX-NeRF</details>  |  [üîó](https://github.com/caiyuanhao1998/SAX-NeRF)
| 23 | [RAO et al.](https://ieeexplore.ieee.org/abstract/document/10004486) |An Energy-Efficient Accelerator for Medical Image  Reconstruction From Implicit Neural Representation|TCAS-I(2022) | CT MRI | NRF | An energy-efficient accelerator for medical image reconstruction from INR and  a  hardware architecture for the INR-based reconstruction algorithm, which co-designs data reuse and computation load| <details><summary>Click</summary>This work presents an energy-efficient accelerator for medical image reconstruction from implicit neural representation (INR). The accelerator implements an INR-based algorithm to deliver high-quality medical image reconstruction with arbitrary resolution from a compact implicit format. In particular, we propose a dedicated hardware architecture based on an optimized computation flow for the INR-based reconstruction algorithm, which co-designs data reuse and computation load. The proposed architecture takes in the coordinate of the intersection of three scans and outputs all the voxel intensities, minimizing the data movement between on-chip and off-chip. To validate the proposed accelerator, we build a proof-of-concept prototype demonstration system using field programmable gate array (FPGA). We also map our design to 40nm CMOS technology to measure the performance of the proposed accelerator. The implementation results show that, running at 400MHz, the proposed accelerator is capable of processing medical images with 256√ó256 resolution in real-time at 26.3 frames per second (FPS), with a power consumption of only 795 mW. Comparison results show that the performance, as well as the energy efficiency of the proposed accelerator, outperforms the central processing unit (CPU)-based and graphic processing unit (GPU)-based implementations. </details>  |  ‚ùå
| 24 | [Feng et al.](https://arxiv.org/pdf/2301.00127) |Spatiotemporal implicit neural representation for unsupervised dynamic MRI reconstruction|	TMI(2025) | CT MRI | NRF | An INR-based unsupervised deep learning method to improve dynamic MRI reconstruction from highly undersampled k-space data, which only takes spatiotemporal coordinates as inputs | <details><summary>Click</summary>Supervised Deep-Learning (DL)-based reconstruction algorithms have shown state-of-the-art results for highly-undersampled dynamic Magnetic Resonance Imaging (MRI) reconstruction. However, the requirement of excessive high-quality ground-truth data hinders their applications due to the generalization problem. Recently, Implicit Neural Representation (INR) has emerged as a powerful DL-based tool for solving the inverse problem by characterizing the attributes of a signal as a continuous function of corresponding coordinates in an unsupervised manner. In this work, we proposed an INR-based method to improve dynamic MRI reconstruction from highly undersampled k-space data, which only takes spatiotemporal coordinates as inputs and does not require any training on external datasets or transfer-learning from prior images. Specifically, the proposed method encodes the dynamic MRI images into neural networks as an implicit function, and the weights of the network are learned from sparsely-acquired (k, t)-space data itself only. Benefiting from the strong implicit continuity regularization of INR together with explicit regularization for low-rankness and sparsity, our proposed method outperforms the compared state-of-the-art methods at various acceleration factors. E.g., experiments on retrospective cardiac cine datasets show an improvement of 0.6‚Äì2.0 dB in PSNR for high accelerations (up to 40.8√ó). The high-quality and inner continuity of the images provided by INR exhibit great potential to further improve the spatiotemporal resolution of dynamic MRI. The code is available at: https://github.com/AMRILab/INR_for_DynamicMRI. </details>  | [üîó]( https://github.com/AMRILab/INR_for_DynamicMRI)   
| 25 | [ImplicitVol](https://arxiv.org/pdf/2109.12108) |ImplicitVol: Sensorless 3D Ultrasound Reconstruction with Deep Implicit Representation|	arXiv(2022) | 3D US | NRF | A sensor-free approach to reconstruct 3D ultrasound volumes from a sparse set of 2D images with deep implicit representation& not available| <details><summary>Click</summary>The objective of this work is to achieve sensorless reconstruction of a 3D volume from a set of 2D freehand ultrasound images with deep implicit representation. In contrast to the conventional way that represents a 3D volume as a discrete voxel grid, we do so by parameterizing it as the zero level-set of a continuous function, i.e. implicitly representing the 3D volume as a mapping from the spatial coordinates to the corresponding intensity values. Our proposed model, termed as ImplicitVol, takes a set of 2D scans and their estimated locations in 3D as input, jointly refining the estimated 3D locations and learning a full reconstruction of the 3D volume. When testing on real 2D ultrasound images, novel cross-sectional views that are sampled from ImplicitVol show significantly better visual quality than those sampled from existing reconstruction approaches, outperforming them by over 30% (NCC and SSIM), between the output and ground-truth on the 3D volume testing data. The code will be made publicly available. </details>  | ‚ùå
| 26 | [NeRP](https://med.stanford.edu/content/dam/sm/dbds/XingSR1.pdf) |NeRP: Implicit Neural Representation Learning with Prior Embedding for Sparsely Sampled Image Reconstruction|	TNNLS(2022) | CT MRI | IPE | A novel deep learning methodology for sparsely sampled medical image reconstruction by learning the implicit neural representation of image with prior embedding| <details><summary>Click</summary>Image reconstruction is an inverse problem that solves for a computational image based on sampled sensor measurement. Sparsely sampled image reconstruction poses additional challenges due to limited measurements. In this work, we propose a methodology of implicit Neural Representation learning with Prior embedding (NeRP) to reconstruct a computational image from sparsely sampled measurements. The method differs fundamentally from previous deep learning-based image reconstruction approaches in that NeRP exploits the internal information in an image prior and the physics of the sparsely sampled measurements to produce a representation of the unknown subject. No large-scale data is required to train the NeRP except for a prior image and sparsely sampled measurements. In addition, we demonstrate that NeRP is a general methodology that generalizes to different imaging modalities such as computed tomography (CT) and magnetic resonance imaging (MRI). We also show that NeRP can robustly capture the subtle yet significant image changes required for assessing tumor progression. </details>  | [üîó]( https://github.com/liyues/NeRP)   
| 27 | [Makkar et al.](https://www.iccr2024.org/papers/526223.pdf) |Partial-ring PET Image Correction using Implicit Neural Representation Learning|	ICCR(2024) | PET | IPE NRF | A deep learning-based approach to enhance image quality in partial-ring PET scanners| <details><summary>Click</summary>This work introduces a deep learning-based approach to enhance image quality in partial-ring PET scanners,which often suffer from significant artifacts due to incomplete angular field of view. Our method utilizes implicit neural representation (INR) learning with coordinate-based multilayer perceptrons (MLPs) featuring sinusoidal activations. These MLPs parameterize partial-ring PET images, training the patientspecific network to predict fully sampled images from partially sampled ones. This approach was validated using twenty digital brain phantoms, Monte Carlo simulated Derenzo phantom, and experimentally acquired hot rod phantom data from the partial-ring PETITION PET scanner. The results show that the INR learning method significantly improves image quality, achieving a PSNR above 30dB and SSIM over 0.95 for all cases. This method presents a promising solution for enhancing PET images from partial-ring scanners.</details>  | ‚ùå
| 29 | [3DSRNet](https://ieeexplore.ieee.org/abstract/document/10235900) |3DSRNet: 3-D Spine Reconstruction Network Using 2-D Orthogonal X-Ray Images Based on Deep Learning |TIM(2023)| CT | Point |It Use a GAN architecture with novel SRCT and SRTE modules to reconstruct 3D spine CT images from 2D X-ray images accurately |<details><summary>Click</summary>Orthopedic spine disease is one of the most common diseases in the clinic. The diagnosis of spinal orthopedic injury is an important basis for the treatment of spinal orthopedic diseases. Due to the complexity of the spine structure, doctors usually need to rely on orthopedic computed tomography (CT) image data for accurate diagnosis. In some cases, such as poor areas or in emergency situations, it is difficult for doctors to make accurate diagnoses using only 2-D x-ray images due to lack of 3-D imaging equipment or time crunch. Therefore, an approach based on 2-D x-ray images is needed to solve this problem. In this article, a novel 3-D spine reconstruction technique based on 2-D orthogonal x-ray images (3DSRNet) is designed. 3DSRNet uses a generative adversarial network (GAN) architecture and novel modules to make 3-D spine reconstruction more accurate and efficient. Spine reconstruction convolutional neural network (CNN)-transformer framework (SRCT) is employed to effectively integrate local bone surface information and long-range relation spinal structure information. Spine reconstruction texture framework (SRTE) is used to extract spine texture features to enhance the effect of pixel-level reconstruction. Experiments show that 3DSRNet achieves excellent 3-D spine reconstruction results on multiple metrics including peak signal-to-noise ratio (PSNR) (45.4666 dB), structural similarity index (SSIM) (0.8850), cosine similarity (CS) (0.7662), mean absolute error (MAE) (23.6696), mean squared error (MSE) (9016.1044), and learned perceptual image patch similarity (LPIPS) (0.0768).</details>  |  ‚ùå
| 30 | [EMISR](https://www.sciencedirect.com/science/article/pii/S0169260719312416?via%3Dihub) |Super-resolution reconstruction of knee magnetic resonance imaging based on deep learning |CMBP(2020)| MRI | Volume |  Utilized three hidden layers from super-resolution convolution neural network  and a sub - pixel convolution layer from efficient sub-pixel convolution neural network |<details><summary>Click</summary>Background and objective With the rapid development of medical imaging and intelligent diagnosis, artificial intelligence methods have become a research hotspot of radiography processing technology in recent years. The low definition of knee magnetic resonance image texture seriously affects the diagnosis of knee osteoarthritis. This paper presents a super-resolution reconstruction method to address this problem. Methods In this paper, we propose an efficient medical image super-resolution (EMISR) method, in which we mainly adopted three hidden layers of super-resolution convolution neural network (SRCNN) and a sub-pixel convolution layer of efficient sub-pixel convolution neural network (ESPCN). The addition of the efficient sub-pixel convolutional layer in the hidden layer and the small network replacement consisting of concatenated convolutions to address low-resolution images but not high-resolution images are important. The EMISR method also uses cascaded small convolution kernels to improve reconstruction speed and deepen the convolution neural network to improve reconstruction quality. Results The proposed method is tested in the public dataset IDI, and the reconstruction quality of the algorithm is higheJMIr than that of the sparse coding-based network (SCN) method, the SRCNN method, and the ESPCN method (+‚ÄØ2.306‚ÄØdB, +‚ÄØ2.540‚ÄØdB, +‚ÄØ1.089‚ÄØdB improved); moreover, the reconstruction speed is faster than its counterparts (+‚ÄØ4.272‚ÄØs, +‚ÄØ1.967‚ÄØs, and +‚ÄØ0.073‚ÄØs improved). Conclusion The experimental results show that our EMISR framework has improved performance and greatly reduces the number of parameters and training time. Furthermore, the reconstructed image presents more details, and the edges are more complete. Therefore, the EMISR technique provides a more powerful medical analysis in knee osteoarthritis examinations.</details>  |  ‚ùå
| 31 | [HDnet](https://ieeexplore.ieee.org/abstract/document/9153172) |Hybrid-domain neural network processing for sparse-view CT reconstruction |TRPMS(2020)| CT | Volume | A hybrid-domain neural network processing for SVCT reconstruction, which can be applied to cone-beam CT imaging without large memory requirements. |<details><summary>Click</summary>X-ray computed tomography (CT) is one of the most widely used tools in medical imaging, industrial nondestructive testing, lesion detection, and other applications. However, decreasing the projection number to lower the X-ray radiation dose usually leads to severe streak artifacts. To improve the quality of the images reconstructed from sparse-view projection data, we developed a hybrid-domain neural network (HDNet) processing for sparse-view CT (SVCT) reconstruction in this study. The HDNet decomposes the SVCT reconstruction problem into two stages and each stage focuses on one mission, which reduces the learning difficulty of the entire network. Experiments based on the simulated and clinical datasets are performed to demonstrate the performance of the proposed method. Compared with other competitive algorithms, quantitative and qualitative results show that the proposed method makes a great improvement on artifact suppression, tiny structure restoration, and contrast retention.</details> |  ‚ùå
| 32 | [Du et al.](https://www.sciencedirect.com/science/article/pii/S0925231219304771) |Super-resolution reconstruction of single anisotropic 3D MR images using residual convolutional neural network|Neurocomput.(2020)| MRI | Volume |A novel CNN-based anisotropic MR image reconstruction method based on residual learning with long and short skip connections |<details><summary>Click</summary> High-resolution (HR) magnetic resonance (MR) imaging is an important diagnostic technique in clinical practice. However, hardware limitations and time constraints often result in the acquisition of anisotropic MR images. It is highly desirable but very challenging to enhance image spatial resolution in medical image analysis for disease diagnosis. Recently, studies have shown that deep convolutional neural networks (CNN) can significantly boost the performance of MR image super-resolution (SR) reconstruction. In this paper, we present a novel CNN-based anisotropic MR image reconstruction method based on residual learning with long and short skip connections. The proposed network can effectively alleviate the vanishing gradient problem of deep networks and learn to restore high-frequency details of MR images. To reduce computational complexity and memory usage, the proposed network utilizes cross-plane self-similarity of 3D T1-weighted (T1w) MR images. Based on experiments on simulated and clinical brain MR images, we demonstrate that the proposed network can significantly improve the spatial resolution of anisotropic MR images with high computational efficiency. The network trained on T1w MR images is able to effectively reconstruct both SR T1w and T2-weighted (T2w) images, exploiting image features for multi-modality reconstruction. Moreover, the experimental results show that the proposed method outperforms classical interpolation methods, non-local means method (NLM), and sparse coding based algorithm in terms of peak signal-to-noise-ratio, structural similarity image index, intensity profile, and small structures. The proposed method can be efficiently applied to SR reconstruction of thick-slice MR images in the out-of-plane views for radiological assessment and post-acquisition processing.</details> |  ‚ùå
| 33 | [DirectPet](https://www.spiedigitallibrary.org/journals/journal-of-medical-imaging/volume-7/issue-3/032503/DirectPET--full-size-neural-network-PET-reconstruction-from-sinogram/10.1117/1.JMI.7.3.032503.full) |DirectPET: full-size neural network PET reconstruction from sinogram data|JMI(2020)| PET | Volume | DirectPET, which is capable of reconstructing multislice image volumes from sinograms |<details><summary>Click</summary>Purpose: Neural network image reconstruction directly from measurement data is a relatively new field of research, which until now has been limited to producing small single-slice images (e.g., 1‚Äâ‚Äâ√ó‚Äâ‚Äâ128‚Äâ‚Äâ√ó‚Äâ‚Äâ128). We proposed a more efficient network design for positron emission tomography called DirectPET, which is capable of reconstructing multislice image volumes (i.e., 16‚Äâ‚Äâ√ó‚Äâ‚Äâ400‚Äâ‚Äâ√ó‚Äâ‚Äâ400) from sinograms. Approach: Large-scale direct neural network reconstruction is accomplished by addressing the associated memory space challenge through the introduction of a specially designed Radon inversion layer. Using patient data, we compare the proposed method to the benchmark ordered subsets expectation maximization (OSEM) algorithm using signal-to-noise ratio, bias, mean absolute error, and structural similarity measures. In addition, line profiles and full-width half-maximum measurements are provided for a sample of lesions. Results: DirectPET is shown capable of producing images that are quantitatively and qualitatively similar to the OSEM target images in a fraction of the time. We also report on an experiment where DirectPET is trained to map low-count raw data to normal count target images, demonstrating the method‚Äôs ability to maintain image quality under a low-dose scenario. Conclusion: The ability of DirectPET to quickly reconstruct high-quality, multislice image volumes suggests potential clinical viability of the method. However, design parameters and performance boundaries need to be fully established before adoption can be considered.[/details] |  ‚ùå
| 34 | [Singh et al.](https://ajronline.org/doi/pdf/10.2214/AJR.19.21809) |Image Quality and Lesion  Detection on Deep Learning  Reconstruction and Iterative  Reconstruction of Submillisievert  Chest and Abdominal CT| AJR(2020) | CT | Volume | At  submillisievert chest and abdominopelvic CT doses, DLR enables image quality and lesion detection superior to commercial IR and FBP images |<details><summary>Click</summary>OBJECTIVE. The objective of this study was to compare image quality and clinically significant lesion detection on deep learning reconstruction (DLR) and iterative reconstruction (IR) images of submillisievert chest and abdominopelvic CT. MATERIALS AND METHODS. Our prospective multiinstitutional study included 59 adult patients (33 women, 26 men; mean age ¬± SD, 65 ¬± 12 years old; mean body mass index [weight in kilograms divided by the square of height in meters] = 27 ¬± 5) who underwent routine chest (n = 22; 16 women, six men) and abdominopelvic (n = 37; 17 women, 20 men) CT on a 640-MDCT scanner (Aquilion ONE, Canon Medical Systems). All patients gave written informed consent for the acquisition of low-dose (LD) CT (LDCT) after a clinically indicated standard-dose (SD) CT (SDCT). The SDCT series (120 kVp, 164‚Äì644 mA) were reconstructed with interactive reconstruction (IR) (adaptive iterative dose reduction [AIDR] 3D, Canon Medical Systems), and the LDCT (100 kVp, 120 kVp; 30‚Äì50 mA) were reconstructed with filtered back-projection (FBP), IR (AIDR 3D and forward-projected model-based iterative reconstruction solution [FIRST], Canon Medical Systems), and deep learning reconstruction (DLR) (Advanced Intelligent Clear-IQ Engine [AiCE], Canon Medical Systems). Four subspecialty-trained radiologists first read all LD image sets and then compared them side-by-side with SD AIDR 3D images in an independent, randomized, and blinded fashion. Subspecialty radiologists assessed image quality of LDCT images on a 3-point scale (1 = unacceptable, 2 = suboptimal, 3 = optimal). Descriptive statistics were obtained, and the Wilcoxon sign rank test was performed. RESULTS. Mean volume CT dose index and dose-length product for LDCT (2.1 ¬± 0.8 mGy, 49 ¬± 13mGy¬∑cm) were lower than those for SDCT (13 ¬± 4.4 mGy, 567 ¬± 249 mGy¬∑cm) (p < 0.0001). All 31 clinically significant abdominal lesions were seen on SD AIDR 3D and LD DLR images. Twenty-five, 18, and seven lesions were detected on LD AIDR 3D, LD FIRST, and LD FBP images, respectively. All 39 pulmonary nodules detected on SD AIDR 3D images were also noted on LD DLR images. LD DLR images were deemed acceptable for interpretation in 97% (35/37) of abdominal and 95‚Äì100% (21‚Äì22/22) of chest LDCT studies (p = 0.2‚Äì0.99). The LD FIRST, LD AIDR 3D, and LD FBP images had inferior image quality compared with SD AIDR 3D images (p < 0.0001). CONCLUSION. At submillisievert chest and abdominopelvic CT doses, DLR enables image quality and lesion detection superior to commercial IR and FBP images.</details> |  ‚ùå
| 35 | [MADGAN](https://link.springer.com/content/pdf/10.1186/s12859-020-03936-1.pdf) |MADGAN: unsupervised medical anomaly detection GAN using multiple adjacent brain MRI slice reconstruction| BMC(2021) | MRI | Volume  | GAN-based multiple adjacent brain MRI slice reconstruction to detect brain anomalies at different stages on multi-sequence structural MRI |<details><summary>Click</summary>Background Unsupervised learning can discover various unseen abnormalities, relying on large-scale unannotated medical images of healthy subjects. Towards this, unsupervised methods reconstruct a 2D/3D single medical image to detect outliers either in the learned feature space or from high reconstruction loss. However, without considering continuity between multiple adjacent slices, they cannot directly discriminate diseases composed of the accumulation of subtle anatomical anomalies, such as Alzheimer‚Äôs disease (AD). Moreover, no study has shown how unsupervised anomaly detection is associated with either disease stages, various (i.e., more than two types of) diseases, or multi-sequence magnetic resonance imaging (MRI) scans. Results We propose unsupervised medical anomaly detection generative adversarial network (MADGAN), a novel two-step method using GAN-based multiple adjacent brain MRI slice reconstruction to detect brain anomalies at different stages on multi-sequence structural MRI: (Reconstruction) Wasserstein loss with Gradient Penalty + 100  loss‚Äîtrained on 3 healthy brain axial MRI slices to reconstruct the next 3 ones‚Äîreconstructs unseen healthy/abnormal scans; (Diagnosis) Average  loss per scan discriminates them, comparing the ground truth/reconstructed slices. For training, we use two different datasets composed of 1133 healthy T1-weighted (T1) and 135 healthy contrast-enhanced T1 (T1c) brain MRI scans for detecting AD and brain metastases/various diseases, respectively. Our self-attention MADGAN can detect AD on T1 scans at a very early stage, mild cognitive impairment (MCI), with area under the curve (AUC) 0.727, and AD at a late stage with AUC 0.894, while detecting brain metastases on T1c scans with AUC 0.921. Conclusions Similar to physicians‚Äô way of performing a diagnosis, using massive healthy training data, our first multiple MRI slice reconstruction approach, MADGAN, can reliably predict the next 3 slices from the previous 3 ones only for unseen healthy images. As the first unsupervised various disease diagnosis, MADGAN can reliably detect the accumulation of subtle anatomical anomalies and hyper-intense enhancing lesions, such as (especially late-stage) AD and brain metastases on multi-sequence MRI scans.</details> |  ‚ùå
| 36 | [FreeSeed](https://arxiv.org/pdf/2307.05890) |FreeSeed: Frequency-Band-Aware and Self-guided Network for Sparse-View CT Reconstruction| MICCAI(2023) | CT | Volume | Propose a simple yet effective FREquency-band-awarE and SElf-guidED network, termed FreeSeed, which can effectively remove artifacts and recover missing details from the contaminated sparse-view CT images. |<details><summary>Click</summary>Sparse-view computed tomography (CT) is a promising solution for expediting the scanning process and mitigating radiation exposure to patients, the reconstructed images, however, contain severe streak artifacts, compromising subsequent screening and diagnosis. Recently, deep learning-based image post-processing methods along with their dual-domain counterparts have shown promising results. However, existing methods usually produce over-smoothed images with loss of details due to i) the difficulty in accurately modeling the artifact patterns in the image domain, and ii) the equal treatment of each pixel in the loss function. To address these issues, we concentrate on the image post-processing and propose a simple yet effective FREquency-band-awarE and SElf-guidED network, termed FreeSeed, which can effectively remove artifacts and recover missing details from the contaminated sparse-view CT images. Specifically, we first propose a frequency-band-aware artifact modeling network (FreeNet), which learns artifact-related frequency-band attention in the Fourier domain for better modeling the globally distributed streak artifact on the sparse-view CT images. We then introduce a self-guided artifact refinement network (SeedNet), which leverages the predicted artifact to assist FreeNet in continuing to refine the severely corrupted details. Extensive experiments demonstrate the superior performance of FreeSeed and its dual-domain counterpart over the state-of-the-art sparse-view CT reconstruction methods. Source code is made available at https://github.com/Masaaki-75/freeseed.</details> |  [üîó](https://github.com/Masaaki-75/freeseed)
| 37 | [Tan et al.](https://link.springer.com/content/pdf/10.1007/s13755-021-00140-0.pdf) |Classification of COVID-19 pneumonia from chest CT images based on reconstructed super-resolution images and VGG neural network |  Health Inf Sci SystÔºà2021Ôºâ| CT | Volume | Utilize SRGAN to reconstruct super - resolution images from original chest CT images, classifying COVID-19 and non-COVID-19 images with VGG16 from the super - resolution images, and validating and comparing the method's performance using the public COVID - CT dataset |<details><summary>Click</summary>The COVID-19 coronavirus has spread rapidly around the world and has caused global panic. Chest CT images play a major role in confirming positive COVID-19 patients. The computer aided diagnosis of COVID-19 from CT images based on artificial intelligence have been developed and deployed in some hospitals. But environmental influences and the movement of lung will affect the image quality, causing the lung parenchyma and pneumonia area unclear in CT images. Therefore, the performance of COVID-19‚Äôs artificial intelligence diagnostic algorithm is reduced. If chest CT images are reconstructed, the accuracy and performance of the aided diagnostic algorithm may be improved. In this paper, a new aided diagnostic algorithm for COVID-19 based on super-resolution reconstructed images and convolutional neural network is presented. Firstly, the SRGAN neural network is used to reconstruct super-resolution images from original chest CT images. Then COVID-19 images and Non-COVID-19 images are classified from super-resolution chest CT images by VGG16 neural network. Finally, the performance of this method is verified by the pubic COVID-CT dataset and compared with other aided diagnosis methods of COVID-19. The experimental results show that improving the data quality through SRGAN neural network can greatly improve the final classification accuracy when the data quality is low. This proves that this method can obtain high accuracy, sensitivity and specificity in the examined test image datasets and has similar performance to other state-of-the-art deep learning aided algorithms.</details> |  ‚ùå
| 38 | [DLR](https://pubs.rsna.org/doi/pdf/10.1148/radiol.2020202317) |Improving Image Quality and Reducing Radiation Dose for Pediatric CT by Using Deep Learning Reconstruction |Radiology(2020)| CT | Volume | To investigate a DLR algorithm‚Äôs dose reduction and image quality improvement for pediatric CT |<details><summary>Click</summary>Background CT deep learning reconstruction (DLR) algorithms have been developed to remove image noise. How the DLR affects image quality and radiation dose reduction has yet to be fully investigated. Purpose To investigate a DLR algorithm‚Äôs dose reduction and image quality improvement for pediatric CT. Materials and Methods DLR was compared with filtered back projection (FBP), statistical-based iterative reconstruction (SBIR), and model-based iterative reconstruction (MBIR) in a retrospective study by using data from CT examinations of pediatric patients (February to December 2018). A comparison of object detectability for 15 objects (diameter, 0.5‚Äì10 mm) at four contrast difference levels (50, 150, 250, and 350 HU) was performed by using a non-prewhitening-matched mathematical observer model with eye filter (d‚ÄôNPWE), task transfer function, and noise power spectrum analysis. Object detectability was assessed by using area under the curve analysis. Three pediatric radiologists performed an observer study to assess anatomic structures with low object-to-background signal and contrast to noise in the azygos vein, right hepatic vein, common bile duct, and superior mesenteric artery. Observers rated from 1 to 10 (worst to best) for edge definition, quantum noise level, and object conspicuity. Analysis of variance and Tukey honest significant difference post hoc tests were used to analyze differences between reconstruction algorithms. Results Images from 19 patients (mean age, 11 years ¬± 5 [standard deviation]; 10 female patients) were evaluated. Compared with FBP, SBIR, and MBIR, DLR demonstrated improved object detectability by 51% (16.5 of 10.9), 18% (16.5 of 13.9), and 11% (16.5 of 14.8), respectively. DLR reduced image noise without noise texture effects seen with MBIR. Radiologist ratings were 7 ¬± 1 (DLR), 6.2 ¬± 1 (MBIR), 6.2 ¬± 1 (SBIR), and 4.6 ¬± 1 (FBP); two-way analysis of variance showed a difference on the basis of reconstruction type (P < .001). Radiologists consistently preferred DLR images (intraclass correlation coefficient, 0.89; 95% CI: 0.83, 0.93). DLR demonstrated 52% (1 of 2.1) greater dose reduction than SBIR. Conclusion The DLR algorithm improved image quality and dose reduction without sacrificing noise texture and spatial resolution.</details> |  ‚ùå
| 39 | [DIOR](https://ieeexplore.ieee.org/abstract/document/9698177) |DIOR: Deep Iterative Optimization-Based Residual-Learning for Limited-Angle CT Reconstruction | TMI(2022)| CT | Volume | The DIOR combines iterative optimization and deep learning based on the residual domain |<details><summary>Click</summary>Limited-angle CT is a challenging problem in real applications. Incomplete projection data will lead to severe artifacts and distortions in reconstruction images. To tackle this problem, we propose a novel reconstruction framework termed Deep Iterative Optimization-based Residual-learning (DIOR) for limited-angle CT. Instead of directly deploying the regularization term on image space, the DIOR combines iterative optimization and deep learning based on the residual domain, significantly improving the convergence property and generalization ability. Specifically, the asymmetric convolutional modules are adopted to strengthen the feature extraction capacity in smooth regions for deep priors. Besides, in our DIOR method, the information contained in low-frequency and high-frequency components is also evaluated by perceptual loss to improve the performance in tissue preservation. Both simulated and clinical datasets are performed to validate the performance of DIOR. Compared with existing competitive algorithms, quantitative and qualitative results show that the proposed method brings a promising improvement in artifact removal, detail restoration and edge preservation.</details> |   ‚ùå
| 40 | [Shaul et al.](https://www.sciencedirect.com/science/article/pii/S1361841520301110) |Subsampled brain MRI reconstruction by generative adversarial neural networks |MIA(2020) | MRI | Volume | The proposed framework exploits the strengths of the U-Net and the GAN architectures for high-quality MRI reconstruction |<details><summary>Click</summary>A main challenge in magnetic resonance imaging (MRI) is speeding up scan time. Beyond improving patient experience and reducing operational costs, faster scans are essential for time-sensitive imaging, such as fetal, cardiac, or functional MRI, where temporal resolution is important and target movement is unavoidable, yet must be reduced. Current MRI acquisition methods speed up scan time at the expense of lower spatial resolution and costlier hardware. We introduce a practical, software-only framework, based on deep learning, for accelerating MRI acquisition, while maintaining anatomically meaningful imaging. This is accomplished by MRI subsampling followed by estimating the missing k-space samples via generative adversarial neural networks. A generator-discriminator interplay enables the introduction of an adversarial cost in addition to fidelity and image-quality losses used for optimizing the reconstruction. Promising reconstruction results are obtained from feasible sampling patterns of up to a fivefold acceleration of diverse brain MRIs, from a large publicly available dataset of healthy adult scans as well as multimodal acquisitions of multiple sclerosis patients and dynamic contrast-enhanced MRI (DCE-MRI) sequences of stroke and tumor patients. Clinical usability of the reconstructed MRI scans is assessed by performing either lesion or healthy tissue segmentation and comparing the results to those obtained by using the original, fully sampled images. Reconstruction quality and usability of the DCE-MRI sequences is demonstrated by calculating the pharmacokinetic (PK) parameters. The proposed MRI reconstruction approach is shown to outperform state-of-the-art methods for all datasets tested in terms of the peak signal-to-noise ratio (PSNR), the structural similarity index (SSIM), as well as either the mean squared error (MSE) with respect to the PK parameters, calculated for the fully sampled DCE-MRI sequences, or the segmentation compatibility, measured in terms of Dice scores and Hausdorff distance. The code is available on GitHub.</details> |  [üîó](https://github.com/ItamarDavid/Subsampled-Brain-MRI-Reconstruction-by-Generative-Adversarial-Neural-Networks.git) 
| 41 | [Pezzotti et al.](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9241039)|An Adaptive Intelligence Algorithm for Undersampled Knee MRI Reconstruction | IEEE(2020) | MRI | Volume | The application of adaptive intelligence to accelerate MR acquisition |<details><summary>Click</summary>Adaptive intelligence aims at empowering machine learning techniques with the additional use of domain knowledge. In this work, we present the application of adaptive intelligence to accelerate MR acquisition. Starting from undersampled k-space data, an iterative learning-based reconstruction scheme inspired by compressed sensing theory is used to reconstruct the images. We developed a novel deep neural network to refine and correct prior reconstruction assumptions given the training data. The network was trained and tested on a knee MRI dataset from the 2019 fastMRI challenge organized by Facebook AI Research and NYU Langone Health. All submissions to the challenge were initially ranked based on similarity with a known groundtruth, after which the top 4 submissions were evaluated radiologically. Our method was evaluated by the fastMRI organizers on an independent challenge dataset. It ranked #1, shared #1, and #3 on respectively the 8√ó accelerated multi-coil, the 4√ó multi-coil, and the 4√ó single-coil tracks. This demonstrates the superior performance and wide applicability of the method.</details> |  ‚ùå
| 42 | [T-net](https://www.sciencedirect.com/science/article/abs/pii/S0730725X1930092X) |Incorporating prior knowledge via volumetric deep residual network to optimize the reconstruction of sparsely sampled MRI | MRI(2020) | MRI | Volume |The Volume Depth Residual Network realizes the mapping between undersampled images and high-quality MR images |<details><summary>Click</summary>For sparse sampling that accelerates magnetic resonance (MR) image acquisition, non-linear reconstruction algorithms have been developed, which incorporated patient specific a prior information. More generic a prior information could be acquired via deep learning and utilized for image reconstruction. In this study, we developed a volumetric hierarchical deep residual convolutional neural network, referred to as T-Net, to provide a data-driven end-to-end mapping from sparsely sampled MR images to fully sampled MR images, where cartilage MR images were acquired using an Ultra-short TE sequence and retrospectively undersampled using pseudo-random Cartesian and radial acquisition schemes. The network had a hierarchical architecture that promoted the sparsity of feature maps and increased the receptive field, which were valuable for signal synthesis and artifact suppression. Relatively dense local connections and global shortcuts were established to facilitate residual learning and compensate for details lost in hierarchical processing. Additionally, volumetric processing was adopted to fully exploit spatial continuity in three-dimensional space. Data consistency was further enforced. The network was trained with 336 three-dimensional images (each consisting of 32 slices) and tested by 24 images. The incorporation of a priori information acquired via deep learning facilitated high acceleration factors (as high as 8) while maintaining high image fidelity (quantitatively evaluated using the structural similarity index measurement). The proposed T-Net had an improved performance as compared to several state-of-the-art networks.</details> | ‚ùå
 | 43 | [Mostafapour et al.](https://www.hug.ch/sites/interhug/files/structures/pinlab/documents/jcde2022.pdf) |Deep learning-guided attenuation correction in the image domain for myocardial perfusion SPECT imaging | JCDE(2022) | SPECT | Volume | Investigate the accuracy of direct attenuation correction in the image domain for myocardial perfusion single-photon emission computed tomography imaging using residual and UNet deep convolutional neural networks. |<details><summary>Click</summary>We investigate the accuracy of direct attenuation correction (AC) in the image domain for myocardial perfusion SPECT (single-photon emission computed tomography) imaging (MPI-SPECT) using residual (ResNet) and UNet deep convolutional neural networks. MPI-SPECT 99mTc-sestamibi images of 99 patients were retrospectively included. UNet and ResNet networks were trained using non-attenuation-corrected SPECT images as input, whereas CT-based attenuation-corrected (CT-AC) SPECT images served as reference. Chang‚Äôs calculated AC approach considering a uniform attenuation coefficient within the body contour was also implemented. Clinical and quantitative evaluations of the proposed methods were performed considering SPECT CT-AC images of 19 subjects (external validation set) as reference. Image-derived metrics, including the voxel-wise mean error (ME), mean absolute error, relative error, structural similarity index (SSI), and peak signal-to-noise ratio, as well as clinical relevant indices, such as total perfusion deficit (TPD), were utilized. Overall, AC SPECT images generated using the deep learning networks exhibited good agreement with SPECT CT-AC images, substantially outperforming Chang‚Äôs method. The ResNet and UNet models resulted in an ME of ‚àí6.99 ¬± 16.72 and ‚àí4.41 ¬± 11.8 and an SSI of 0.99 ¬± 0.04 and 0.98 ¬± 0.05, respectively. Chang‚Äôs approach led to ME and SSI of 25.52 ¬± 33.98 and 0.93 ¬± 0.09, respectively. Similarly, the clinical evaluation revealed a mean TPD of 12.78 ¬± 9.22% and 12.57 ¬± 8.93% for ResNet and UNet models, respectively, compared to 12.84 ¬± 8.63% obtained from SPECT CT-AC images. Conversely, Chang‚Äôs approach led to a mean TPD of 16.68 ¬± 11.24%. The deep learning AC methods have the potential to achieve reliable AC in MPI-SPECT imaging.</details> |  ‚ùå
 | 44 | [TPL-CNN](https://link.springer.com/content/pdf/10.1186/s40658-024-00687-3.pdf) |SPECT-MPI iterative denoising during the reconstruction process using a two-phase learned convolutional neural network | EJNMMI(2024) | SPECT | Volume | Use GAN as a denoising network after each iteration of the Ordered Subsets Expectation Maximization algorithm|<details><summary>Click</summary>PurposeThe problem of image denoising in single-photon emission computed tomography (SPECT) myocardial perfusion imaging (MPI) is a fundamental challenge. Although various image processing techniques have been presented, they may degrade the contrast of denoised images. The proposed idea in this study is to use a deep neural network as the denoising procedure during the iterative reconstruction process rather than the post-reconstruction phase. This method could decrease the background coefficient of variation (COV_bkg) of the final reconstructed image, which represents the amount of random noise, while improving the contrast-to-noise ratio (CNR).MethodsIn this study, a generative adversarial network is used, where its generator is trained by a two-phase approach. In the first phase, the network is trained by a confined image region around the heart in transverse view. The second phase improves the network‚Äôs generalization by tuning the network weights with the full image size as the input. The network was trained and tested by a dataset of 247 patients who underwent two immediate serially high- and low-noise SPECT-MPI.ResultsQuantitative results show that compared to post-reconstruction low pass filtering and post-reconstruction deep denoising methods, our proposed method can decline the COV_bkg of the images by up to 10.28% and 12.52% and enhance the CNR by up to 54.54% and 45.82%, respectively.ConclusionThe iterative deep denoising method outperforms 2D low-pass Gaussian filtering with an 8.4-mm FWHM and post-reconstruction deep denoising approaches.</details> |   [üîó](https://github.com/FYousefzadeh/in-house-simple-OSEM2D-code.git)
 | 45 | [DLE](https://link.springer.com/content/pdf/10.1007/s00259-021-05478-x.pdf) |Image enhancement of whole-body oncology [18F]-FDG PET scans using deep neural networks to reduce noise | EJNMMI(2022) | PET | Volume |The DLE was trained to map full- and partial-duration OSEM images (with TOF and PSF modelling) to full duration BSREM images |<details><summary>Click</summary>PurposeTo enhance the image quality of oncology [18F]-FDG PET scans acquired in shorter times and reconstructed by faster algorithms using deep neural networks.MethodsList-mode data from 277 [18F]-FDG PET/CT scans, from six centres using GE Discovery PET/CT scanners, were split into ¬æ-, ¬Ω- and ¬º-duration scans. Full-duration datasets were reconstructed using the convergent block sequential regularised expectation maximisation (BSREM) algorithm. Short-duration datasets were reconstructed with the faster OSEM algorithm. The 277 examinations were divided into training (n‚Äâ=‚Äâ237), validation (n‚Äâ=‚Äâ15) and testing (n‚Äâ=‚Äâ25) sets. Three deep learning enhancement (DLE) models were trained to map full and partial-duration OSEM images into their target full-duration BSREM images. In addition to standardised uptake value (SUV) evaluations in lesions, liver and lungs, two experienced radiologists scored the quality of testing set images and BSREM in a blinded clinical reading (175 series).ResultsOSEM reconstructions demonstrated up to 22% difference in lesion SUVmax, for different scan durations, compared to full-duration BSREM. Application of the DLE models reduced this difference significantly for full-, ¬æ- and ¬Ω-duration scans, while simultaneously reducing the noise in the liver. The clinical reading showed that the standard DLE model with full- or ¬æ-duration scans provided an image quality substantially comparable to full-duration scans with BSREM reconstruction, yet in a shorter reconstruction time.ConclusionDeep learning‚Äìbased image enhancement models may allow a reduction in scan time (or injected activity) by up to 50%, and can decrease reconstruction time to a third, while maintaining image quality.</details> |  ‚ùå
 | 46 | [CINE](https://www.nature.com/articles/s41598-020-70551-8.pdf) |CINENet: deep learning-based 3D cardiac CINE MRI reconstruction with multi-coil complex-valued 4D spatio-temporal convolutions | nature(2020) | MRI | Volume |Multi-coil complex-valued 4D (3D$+$time) deep-learning based MR reconstruction for highly prospectively undersampled 3D Cartesian cardiac CINE data. |<details><summary>Click</summary>Cardiac CINE magnetic resonance imaging is the gold-standard for the assessment of cardiac function. Imaging accelerations have shown to enable 3D CINE with left ventricular (LV) coverage in a single breath-hold. However, 3D imaging remains limited to anisotropic resolution and long reconstruction times. Recently deep learning has shown promising results for computationally efficient reconstructions of highly accelerated 2D CINE imaging. In this work, we propose a novel 4D (3D‚Äâ+‚Äâtime) deep learning-based reconstruction network, termed 4D CINENet, for prospectively undersampled 3D Cartesian CINE imaging. CINENet is based on (3‚Äâ+‚Äâ1)D complex-valued spatio-temporal convolutions and multi-coil data processing. We trained and evaluated the proposed CINENet on in-house acquired 3D CINE data of 20 healthy subjects and 15 patients with suspected cardiovascular disease. The proposed CINENet network outperforms iterative reconstructions in visual image quality and contrast (+‚Äâ67% improvement). We found good agreement in LV function (bias‚Äâ¬±‚Äâ95% confidence) in terms of end-systolic volume (0‚Äâ¬±‚Äâ3.3 ml), end-diastolic volume (‚àí‚Äâ0.4‚Äâ¬±‚Äâ2.0 ml) and ejection fraction (0.1‚Äâ¬±‚Äâ3.2%) compared to clinical gold-standard 2D CINE, enabling single breath-hold isotropic 3D CINE in less than 10 s scan and‚Äâ~‚Äâ5 s reconstruction time.</details> |  ‚ùå
 | 47 | [Lin et al.](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2021.646013/full) |Bidirectional Mapping of Brain MRI and PET With 3D Reversible GAN for the Diagnosis of Alzheimer‚Äôs Disease | FIN(2021) | MRI PET | Volume |Bidirectional Mapping of Brain MRI and PET With 3D Reversible GAN |<details><summary>Click</summary>Combining multi-modality data for brain disease diagnosis such as Alzheimer‚Äôs disease (AD) commonly leads to improved performance than those using a single modality. However, it is still challenging to train a multi-modality model since it is difficult in clinical practice to obtain complete data that includes all modality data. Generally speaking, it is difficult to obtain both magnetic resonance images (MRI) and positron emission tomography (PET) images of a single patient. PET is expensive and requires the injection of radioactive substances into the patient‚Äôs body, while MR images are cheaper, safer, and more widely used in practice. Discarding samples without PET data is a common method in previous studies, but the reduction in the number of samples will result in a decrease in model performance. To take advantage of multi-modal complementary information, we first adopt the Reversible Generative Adversarial Network (RevGAN) model to reconstruct the missing data. After that, a 3D convolutional neural network (CNN) classification model with multi-modality input was proposed to perform AD diagnosis. We have evaluated our method on the Alzheimer‚Äôs Disease Neuroimaging Initiative (ADNI) database, and compared the performance of the proposed method with those using state-of-the-art methods. The experimental results show that the structural and functional information of brain tissue can be mapped well and that the image synthesized by our method is close to the real image. In addition, the use of synthetic data is beneficial for the diagnosis and prediction of Alzheimer‚Äôs disease, demonstrating the effectiveness of the proposed framework.</details> |  ‚ùå
 | 48 | [AUTOMAP](https://www.nature.com/articles/s41598-021-87482-7.pdf) |Boosting the signal-to-noise of low-field MRI with deep learning image reconstruction | nature(2021) | MRI | Volume |Utilize end-to-end deep neural network approach to improve the image quality of highly noise-corrupted low-field MRI data. |<details><summary>Click</summary>Recent years have seen a resurgence of interest in inexpensive low magnetic field (<‚Äâ0.3 T) MRI systems mainly due to advances in magnet, coil and gradient set designs. Most of these advances have focused on improving hardware and signal acquisition strategies, and far less on the use of advanced image reconstruction methods to improve attainable image quality at low field. We describe here the use of our end-to-end deep neural network approach (AUTOMAP) to improve the image quality of highly noise-corrupted low-field MRI data. We compare the performance of this approach to two additional state-of-the-art denoising pipelines. We find that AUTOMAP improves image reconstruction of data acquired on two very different low-field MRI systems: human brain data acquired at 6.5 mT, and plant root data acquired at 47 mT, demonstrating SNR gains above Fourier reconstruction by factors of 1.5- to 4.5-fold, and 3-fold, respectively. In these applications, AUTOMAP outperformed two different contemporary image-based denoising algorithms, and suppressed noise-like spike artifacts in the reconstructed images. The impact of domain-specific training corpora on the reconstruction performance is discussed. The AUTOMAP approach to image reconstruction will enable significant image quality improvements at low-field, especially in highly noise-corrupted environments.<details> |  ‚ùå
 | 49 | [DiffusionMBIR](https://openaccess.thecvf.com/content/CVPR2023/papers/Chung_Solving_3D_Inverse_Problems_Using_Pre-Trained_2D_Diffusion_Models_CVPR_2023_paper.pdf) |Solving 3D Inverse Problems Using Pre-Trained 2D Diffusion Models | CVRP(2023) | CT | Volume |Combine the ideas from the conventional model-based iterative recon-struction with the modern diffusion models |<details><summary>Click</summary>Diffusion models have emerged as the new state-of-the-art generative model with high quality samples, with intriguing properties such as mode coverage and high flexibility. They have also been shown to be effective inverse problem solvers, acting as the prior of the distribution, while the information of the forward model can be granted at the sampling stage. Nonetheless, as the generative process remains in the same high dimensional (i.e. identical to data dimension) space, the models have not been extended to 3D inverse problems due to the extremely high memory and computational cost. In this paper, we combine the ideas from the conventional model-based iterative reconstruction with the modern diffusion models, which leads to a highly effective method for solving 3D medical image reconstruction tasks such as sparse-view tomography, limited angle tomography, compressed sensing MRI from pre-trained 2D diffusion models. In essence, we propose to augment the 2D diffusion prior with a model-based prior in the remaining direction at test time, such that one can achieve coherent reconstructions across all dimensions. Our method can be run in a single commodity GPU, and establishes the new state-of-the-art, showing that the proposed method can perform reconstructions of high fidelity and accuracy even in the most extreme cases (e.g. 2-view 3D tomography). We further reveal that the generalization capacity of the proposed method is surprisingly high, and can be used to reconstruct volumes that are entirely different from the training dataset. Code available: https://github.com/HJ-harry/DiffusionMBIR</details> |  [üîó](https://github.com/HJ-harry/DiffusionMBIR)
| 50 | [Deidda1 et al.](https://link.springer.com/content/pdf/10.1186/s40658-023-00549-4.pdf) |Triple modality image reconstruction of PET data using SPECT, PET, CT information increases lesion uptake in images of patients treated with radioembolization with 90 Y micro-spheres | EJNMMI(2023) | CT PET SPECT | Volume |To include PET, SPECT and CT information in the reconstruction of PET data |<details><summary>Click</summary>PurposeNuclear medicine imaging modalities like computed tomography (CT), single photon emission CT (SPECT) and positron emission tomography (PET) are employed in the field of theranostics to estimate and plan the dose delivered to tumors and the surrounding tissues and to monitor the effect of the therapy. However, therapeutic radionuclides often provide poor images, which translate to inaccurate treatment planning and inadequate monitoring images. Multimodality information can be exploited in the reconstruction to enhance image quality. Triple modality PET/SPECT/CT scanners are particularly useful in this context due to the easier registration process between images. In this study, we propose to include PET, SPECT and CT information in the reconstruction of PET data. The method is applied to Yttrium-90 (90Y) data.MethodsData from a NEMA phantom filled with 90Y were used for validation. PET, SPECT and CT data from 10 patients treated with Selective Internal Radiation Therapy (SIRT) were used. Different combinations of prior images using the Hybrid kernelized expectation maximization were investigated in terms of VOI activity and noise suppression.ResultsOur results show that triple modality PET reconstruction provides significantly higher uptake when compared to the method used as standard in the hospital and OSEM. In particular, using CT-guided SPECT images, as guiding information in the PET reconstruction significantly increases uptake quantification on tumoral lesions.ConclusionThis work proposes the first triple modality reconstruction method and demonstrates up to 69% lesion uptake increase over standard methods with SIRT 90Y patient data. Promising results are expected for other radionuclide combination used in theranostic applications using PET and SPECT.</details> | [üîó](https://github.com/UCL/STIR.git_)
 | 51 | [Recon3DMLP](https://arxiv.org/pdf/2301.08868) |Computationally Efficient 3D MRI Reconstruction with Adaptive MLP | MICCAI(2023) | MRI | Volume |A hybrid of CNN modules with small kernels for low-frequency reconstruction and adaptive MLP (dMLP) modules with large kernels to boost the high-frequency reconstruction |<details><summary>Click</summary>Compared with 2D MRI, 3D MRI provides superior volumetric spatial resolution and signal-to-noise ratio. However, it is more challenging to reconstruct 3D MRI images. Current methods are mainly based on convolutional neural networks (CNN) with small kernels, which are difficult to scale up to have sufficient fitting power for 3D MRI reconstruction due to the large image size and GPU memory constraint. Furthermore, MRI reconstruction is a deconvolution problem, which demands long-distance information that is difficult to capture by CNNs with small convolution kernels. The multi-layer perceptron (MLP) can model such long-distance information, but it requires a fixed input size. In this paper, we proposed Recon3DMLP, a hybrid of CNN modules with small kernels for low-frequency reconstruction and adaptive MLP (dMLP) modules with large kernels to boost the high-frequency reconstruction, for 3D MRI reconstruction. We further utilized the circular shift operation based on MRI physics such that dMLP accepts arbitrary image size and can extract global information from the entire FOV. We also propose a GPU memory efficient data fidelity module that can reduce >50% memory. We compared Recon3DMLP with other CNN-based models on a high-resolution (HR) 3D MRI dataset. Recon3DMLP improves HR 3D reconstruction and outperforms several existing CNN-based models under similar GPU memory consumption, which demonstrates that Recon3DMLP is a practical solution for HR 3D MRI reconstruction.</details> |  [üîó](https://github.com/guopengf/ReconFormer)[üîó](https://github.com/MLI-lab/imaging_MLPs)
| 52 | [Hashimoto et al.](https://arxiv.org/pdf/2212.11844) |Fully 3D implementation of the end-to-end deep image prior-based PET image reconstruction using block iterative algorithm | PMB(2023) | PET | Volume |a shape-aware diffusion model for 3D image reconstruction from limited 2D images, incorporating shape priors to enhance topologyble |<details><summary>Click</summary>Objective. Deep image prior (DIP) has recently attracted attention owing to its unsupervised positron emission tomography (PET) image reconstruction method, which does not require any prior training dataset. In this paper, we present the first attempt to implement an end-to-end DIP-based fully 3D PET image reconstruction method that incorporates a forward-projection model into a loss function. Approach. A practical implementation of a fully 3D PET image reconstruction could not be performed at present because of a graphics processing unit memory limitation. Consequently, we modify the DIP optimization to a block iteration and sequential learning of an ordered sequence of block sinograms. Furthermore, the relative difference penalty (RDP) term is added to the loss function to enhance the quantitative accuracy of the PET image. Main results. We evaluated our proposed method using Monte Carlo simulation with [18F]FDG PET data of a human brain and a preclinical study on monkey-brain [18F]FDG PET data. The proposed method was compared with the maximum-likelihood expectation maximization (EM), maximum a posteriori EM with RDP, and hybrid DIP-based PET reconstruction methods. The simulation results showed that, compared with other algorithms, the proposed method improved the PET image quality by reducing statistical noise and better preserved the contrast of brain structures and inserted tumors. In the preclinical experiment, finer structures and better contrast recovery were obtained with the proposed method. Significance. The results indicated that the proposed method could produce high-quality images without a prior training dataset. Thus, the proposed method could be a key enabling technology for the straightforward and practical implementation of end-to-end DIP-based fully 3D PET image reconstruction.</details> |  ‚ùå
| 53 | [2D-to-3D-MaxiDeform](https://www.sciencedirect.com/science/article/abs/pii/S0010482524013489) |2D-to-3DMaxiDeform: A computer-aided approach for 3D construction of maxillary sinus from PA and lateral X-ray images| CBM(2024)|CT|Point| It use a deformable point cloud template and relative movement factor for accurate 3D maxillary sinus reconstruction from 2D X-ray images|<details><summary>Click</summary>3D volume construction of the maxillary sinus is important for understanding the 3D surface morphology of the maxillary sinuses and detecting changes or obstruction in sinuses. It is important to detect the pathological conditions affecting the sinuses and to determine the treatment outcomes. The cases of sinusitis and various other pathoses in maxillary sinuses are getting comparatively higher than in other sinuses. Therefore, analysis of maxillary sinus structure has some clinical importance. 3D volume construction from Computed Tomography (CT) can help in the volumetric study of the maxillary sinus. Nonetheless, CT imaging is expensive and has a higher radiation dose than X-ray imaging. Thus, the 3D construction from X-ray images is a challenging problem. This paper proposed a 3D construction method for maxillary sinus from lateral and Posterior-anterior (PA) 2D X-ray images. A novel 3D maxillary sinus construction model (2D-to-3D-MaxiDeform) is proposed which evolves the shape and size of the maxillary sinus providing the required volume, and other linear measurements in mesh format. The 2D-to-3DMaxiDeform uses a 3D point cloud template of the maxillary sinus and applies for registration on the template model. The deformation moves the 3D template points to the required positions of the X-ray image contours. The registration of the template model before deformation into the required shape makes sure the size, position, and angle of the template model are mapped with the input X-ray contours. The evaluation of the 2D-to-3DMaxiDeform achieves an average accuracy of 0.83, mDSC (mean Dice similarity coefficient) of 0.80, mIoU (mean Intersection over Union) of 0.67, recall of 0.90, precision of 0.74, specificity of 0.78 and RMSE of 2.3 mm. The proposed method is the first and novel approach for maxillary sinus 3D construction from X-ray images. The result shows that the proposed method could be a valuable tool for generating 3D models of the maxillary sinus to be used in a clinical setting.</details> | ‚ùå
| 54 | [Du et al.](https://www.sciencedirect.com/science/article/abs/pii/S0925231219304771) |Super-resolution reconstruction of single anisotropic 3D MR images using residual convolutional neural network|Neurocomputing(2020)|MRI|Point|a deep learning model using residual learning and skip connections for super-resolution reconstruction of 3D MR images|<details><summary>Click</summary>High-resolution (HR) magnetic resonance (MR) imaging is an important diagnostic technique in clinical practice. However, hardware limitations and time constraints often result in the acquisition of anisotropic MR images. It is highly desirable but very challenging to enhance image spatial resolution in medical image analysis for disease diagnosis. Recently, studies have shown that deep convolutional neural networks (CNN) can significantly boost the performance of MR image super-resolution (SR) reconstruction. In this paper, we present a novel CNN-based anisotropic MR image reconstruction method based on residual learning with long and short skip connections. The proposed network can effectively alleviate the vanishing gradient problem of deep networks and learn to restore high-frequency details of MR images. To reduce computational complexity and memory usage, the proposed network utilizes cross-plane self-similarity of 3D T1-weighted (T1w) MR images. Based on experiments on simulated and clinical brain MR images, we demonstrate that the proposed network can significantly improve the spatial resolution of anisotropic MR images with high computational efficiency. The network trained on T1w MR images is able to effectively reconstruct both SR T1w and T2-weighted (T2w) images, exploiting image features for multi-modality reconstruction. Moreover, the experimental results show that the proposed method outperforms classical interpolation methods, non-local means method (NLM), and sparse coding based algorithm in terms of peak signal-to-noise-ratio, structural similarity image index, intensity profile, and small structures. The proposed method can be efficiently applied to SR reconstruction of thick-slice MR images in the out-of-plane views for radiological assessment and post-acquisition processing.</details> | ‚ùå
| 55 | [Wei et al.](https://aapm.onlinelibrary.wiley.com/doi/abs/10.1002/mp.16141) |Real-time 3D MRI reconstruction from cine-MRI using unsupervised network in MRI-guided radiotherapy for liver cancer|Medical Physics(2022)|MRI|Point|an unsupervised deep learning model for real-time 3D MRI reconstruction from cine-MRI, using deformation vector fields to estimate respiratory motion|<details><summary>Click</summary>Purpose Respiration has a major impact on the accuracy of radiation treatment for thorax and abdominal tumours. Instantaneous volumetric imaging could provide precise knowledge of tumour and normal organs‚Äô three-dimensional (3D) movement, which is the key to reducing the negative effect of breathing motion. Therefore, this study proposed a real-time 3D MRI reconstruction method from cine-MRI using an unsupervised network.Methods and materials Cine-MRI and setup 3D-MRI from eight patients with liver cancer were utilized to establish and validate the deep learning network for 3D-MRI reconstruction. Unlike previous methods that required 4D-MRI for network training, the proposed method utilized a reference 3D-MRI and cine-MRI to generate the training data. Then, a network was trained in an unsupervised manner to estimate the relationship between the cine-MRI acquired on coronal plane and deformation vector field (DVF) that describes the patient's breathing motion. After the training process, the coronal cine-MRI were inputted into the network, and the corresponding DVF was obtained. By wrapping the reference 3D-MRI with the generated DVF, the 3D-MRI could be reconstructed.Results The reconstructed 3D-MRI slices were compared with the corresponding phase-sorted cine-MRI using dice similarity coefficients (DSCs) of liver contours and blood vessel localization error. In all patients, the liver DSC had mean value >96.1% and standard deviation < 1.3%; the blood vessel localization error had mean value <2.6 mm, and standard deviation was <2.0 mm. Moreover, the time for 3D-MRI reconstruction was approximately 100 ms. These results indicated that the proposed method could accurately reconstruct the 3D-MRI in real time.Conclusions The proposed method could accurately reconstruct the 3D-MRI from cine-MRI in real time. This method has great potential in improving the accuracy of radiotherapy for moving tumours.</details> | ‚ùå
| 56 |[PCNN](https://analyticalsciencejournals.onlinelibrary.wiley.com/doi/abs/10.1002/nbm.4405) |Rapid reconstruction of highly undersampled, non-Cartesian real-time cine k-space data using a perceptual complex neural network (PCNN)|NMRB(2021)|MRI|Point|a perceptual complex neural network for rapid reconstruction of undersampled MRI data using complex convolution and perceptual loss|<details><summary>Click</summary>Highly accelerated real-time cine MRI using compressed sensing (CS) is a promising approach to achieve high spatio-temporal resolution and clinically acceptable image quality in patients with arrhythmia and/or dyspnea. However, its lengthy image reconstruction time may hinder its clinical translation. The purpose of this study was to develop a neural network for reconstruction of non-Cartesian real-time cine MRI k-space data faster (<1 min per slice with 80 frames) than graphics processing unit (GPU)-accelerated CS reconstruction, without significant loss in image quality or accuracy in left ventricular (LV) functional parameters. We introduce a perceptual complex neural network (PCNN) that trains on complex-valued MRI signal and incorporates a perceptual loss term to suppress incoherent image details. This PCNN was trained and tested with multi-slice, multi-phase, cine images from 40 patients (20 for training, 20 for testing), where the zero-filled images were used as input and the corresponding CS reconstructed images were used as practical ground truth. The resulting images were compared using quantitative metrics (structural similarity index (SSIM) and normalized root mean square error (NRMSE)) and visual scores (conspicuity, temporal fidelity, artifacts, and noise scores), individually graded on a five-point scale (1, worst; 3, acceptable; 5, best), and LV ejection fraction (LVEF). The mean processing time per slice with 80 frames for PCNN was 23.7 ¬± 1.9 s for pre-processing (Step 1, same as CS) and 0.822 ¬± 0.004 s for dealiasing (Step 2, 166 times faster than CS). Our PCNN produced higher data fidelity metrics (SSIM = 0.88 ¬± 0.02, NRMSE = 0.014 ¬± 0.004) compared with CS. While all the visual scores were significantly different (P < 0.05), the median scores were all 4.0 or higher for both CS and PCNN. LVEFs measured from CS and PCNN were strongly correlated (R2 = 0.92) and in good agreement (mean difference = ‚àí1.4% [2.3% of mean]; limit of agreement = 10.6% [17.6% of mean]). The proposed PCNN is capable of rapid reconstruction (25 s per slice with 80 frames) of non-Cartesian real-time cine MRI k-space data, without significant loss in image quality or accuracy in LV functional parameters.</details> | ‚ùå
| 57 |[CTTR](https://www.sciencedirect.com/science/article/abs/pii/S1120179722020154)|Dual-domain sparse-view CT reconstruction with Transformers|Physica Medica(2022)|CT|Point|a dual-domain deep learning model for sparse-view CT reconstruction, using Transformers to enhance images with sinogram features|<details><summary>Click</summary>Purpose:Computed Tomography (CT) has been widely used in the medical field. Sparse-view CT is an effective and feasible method to reduce the radiation dose. However, the conventional filtered back projection (FBP) algorithm will suffer from severe artifacts in sparse-view CT. Iterative reconstruction algorithms have been adopted to remove artifacts, but they are time-consuming due to repeated projection and back projection and may cause blocky effects. To overcome the difficulty in sparse-view CT, we proposed a dual-domain sparse-view CT algorithm CT Transformer (CTTR) and paid attention to sinogram information.Methods:CTTR treats sinograms as sentences and enhances reconstructed images with sinogram‚Äôs characteristics. We qualitatively evaluate the CTTR, an iterative method TVM-POCS, a convolutional neural network based method FBPConvNet in terms of a reduction in artifacts and a preservation of details. Besides, we also quantitatively evaluate these methods in terms of RMSE, PSNR and SSIM.Results:We evaluate our method on the Lung Image Database Consortium image collection with different numbers of projection views and noise levels. Experiment studies show that, compared with other methods, CTTR can reduce more artifacts and preserve more details on various scenarios. Specifically, CTTR improves the FBPConvNet performance of PSNR by 0.76 dB with 30 projections.Conclusions:The performance of our proposed CTTR is better than the method based on CNN in the case of extremely sparse views both on visual results and quantitative evaluation. Our proposed method provides a new idea for the application of Transformers to CT image processing.</details> | ‚ùå
| 58 |[Hashimoto et al.](https://arxiv.org/pdf/2212.11844)|Fully 3D Implementation of the End-to-end Deep Image Prior-based PET Image Reconstruction Using Block Iterative Algorithm|PM&B(2023)|PET|Volume|a shape-aware diffusion model for 3D image reconstruction from limited 2D images, incorporating shape priors to enhance topologyble|<details><summary>Click</summary>Objective. Deep image prior (DIP) has recently attracted attention owing to its unsupervised positron emission tomography (PET) image reconstruction method, which does not require any prior training dataset. In this paper, we present the first attempt to implement an end-to-end DIP-based fully 3D PET image reconstruction method that incorporates a forward-projection model into a loss function. Approach. A practical implementation of a fully 3D PET image reconstruction could not be performed at present because of a graphics processing unit memory limitation. Consequently, we modify the DIP optimization to a block iteration and sequential learning of an ordered sequence of block sinograms. Furthermore, the relative difference penalty (RDP) term is added to the loss function to enhance the quantitative accuracy of the PET image. Main results. We evaluated our proposed method using Monte Carlo simulation with [18F]FDG PET data of a human brain and a preclinical study on monkey-brain [18F]FDG PET data. The proposed method was compared with the maximum-likelihood expectation maximization (EM), maximum a posteriori EM with RDP, and hybrid DIP-based PET reconstruction methods. The simulation results showed that, compared with other algorithms, the proposed method improved the PET image quality by reducing statistical noise and better preserved the contrast of brain structures and inserted tumors. In the preclinical experiment, finer structures and better contrast recovery were obtained with the proposed method. Significance. The results indicated that the proposed method could produce high-quality images without a prior training dataset. Thus, the proposed method could be a key enabling technology for the straightforward and practical implementation of end-to-end DIP-based fully 3D PET image reconstruction.</details> | ‚ùå
| 59 |[MCAD](https://arxiv.org/pdf/2406.13150)|MCAD: Multi-modal Conditioned Adversarial Diffusion Model for High-Quality PET Image Reconstruction |MICCAI(2024)|PET|Point|a multi-modal conditioned adversarial diffusion model for reconstructing high-quality PET images from low-dose PET and clinical data using diffusion processes and semantic consistency|<details><summary>Click</summary>Radiation hazards associated with standard-dose positron emission tomography (SPET) images remain a concern, whereas the quality of low-dose PET (LPET) images fails to meet clinical requirements. Therefore, there is great interest in reconstructing SPET images from LPET images. However, prior studies focus solely on image data, neglecting vital complementary information from other modalities, e.g., patients‚Äô clinical tabular, resulting in compromised reconstruction with limited diagnostic utility. Moreover, they often overlook the semantic consistency between real SPET and reconstructed images, leading to distorted semantic contexts. To tackle these problems, we propose a novel Multi-modal Conditioned Adversarial Diffusion model (MCAD) to reconstruct SPET images from multi-modal inputs, including LPET images and clinical tabular. Specifically, our MCAD incorporates a Multi-modal conditional Encoder (Mc-Encoder) to extract multi-modal features, followed by a conditional diffusion process to blend noise with multi-modal features and gradually map blended features to the target SPET images. To balance multi-modal inputs, the Mc-Encoder embeds Optimal Multi-modal Transport co-Attention (OMTA) to narrow the heterogeneity gap between image and tabular while capturing their interactions, providing sufficient guidance for reconstruction. In addition, to mitigate semantic distortions, we introduce the Multi-Modal Masked Text Reconstruction (M3TRec), which leverages semantic knowledge extracted from denoised PET images to restore the masked clinical tabular, thereby compelling the network to maintain accurate semantics during reconstruction. To expedite the diffusion process, we further introduce an adversarial diffusive network with a reduced number of diffusion steps. Experiments show that our method achieves the state-of-the-art performance both qualitatively and quantitatively.</details> | ‚ùå
| 60 |[MEaTransGAN](https://www.sciencedirect.com/science/article/abs/pii/S1361841523002438)|3D multi-modality Transformer-GAN for high-quality PET reconstruction|MIA(2024)|PET|Point|a novel deep learning model that integrates CNNs, Transformer, and GANs to reconstruct high-quality standard-dose PET images from low-dose PET and T1-MRI images, enhancing diagnostic value while reducing radiation exposure|<details><summary>Click</summary>Positron emission tomography (PET) scans can reveal abnormal metabolic activities of cells and provide favorable information for clinical patient diagnosis. Generally, standard-dose PET (SPET) images contain more diagnostic information than low-dose PET (LPET) images but higher-dose scans can also bring higher potential radiation risks. To reduce the radiation risk while acquiring high-quality PET images, in this paper, we propose a 3D multi-modality edge-aware Transformer-GAN for high-quality SPET reconstruction using the corresponding LPET images and T1 acquisitions from magnetic resonance imaging (T1-MRI). Specifically, to fully excavate the metabolic distributions in LPET and anatomical structural information in T1-MRI, we first use two separate CNN-based encoders to extract local spatial features from the two modalities, respectively, and design a multimodal feature integration module to effectively integrate the two kinds of features given the diverse contributions of features at different locations. Then, as CNNs can describe local spatial information well but have difficulty in modeling long-range dependencies in images, we further apply a Transformer-based encoder to extract global semantic information in the input images and use a CNN decoder to transform the encoded features into SPET images. Finally, a patch-based discriminator is applied to ensure the similarity of patch-wise data distribution between the reconstructed and real images. Considering the importance of edge information in anatomical structures for clinical disease diagnosis, besides voxel-level estimation error and adversarial loss, we also introduce an edge-aware loss to retain more edge detail information in the reconstructed SPET images. Experiments on the phantom dataset and clinical dataset validate that our proposed method can effectively reconstruct high-quality SPET images and outperform current state-of-the-art methods in terms of qualitative and quantitative metrics.</details> | ‚ùå
| 61 |[X2CT-GAN](https://openaccess.thecvf.com/content_CVPR_2019/papers/Ying_X2CT-GAN_Reconstructing_CT_From_Biplanar_X-Rays_With_Generative_Adversarial_Networks_CVPR_2019_paper.pdf)|X2CT-GAN: Reconstructing CT From Biplanar X-Rays With Generative Adversarial Networks|CVPR(2019)|CT|Point|a GAN-based model that reconstructs high-resolution 3D CT images from biplanar 2D X-rays using a specially designed generator and combined loss functions|<details><summary>Click</summary>Computed tomography (CT) can provide a 3D view of the patient's internal organs, facilitating disease diagnosis, but it incurs more radiation dose to a patient and a CT scanner is much more cost prohibitive than an X-ray machine too. Traditional CT reconstruction methods require hundreds of X-ray projections through a full rotational scan of the body, which cannot be performed on a typical X-ray machine. In this work, we propose to reconstruct CT from two orthogonal X-rays using the generative adversarial network (GAN) framework. A specially designed generator network is exploited to increase data dimension from 2D (X-rays) to 3D (CT), which is not addressed in previous research of GAN. A novel feature fusion method is proposed to combine information from two X-rays. The mean squared error (MSE) loss and adversarial loss are combined to train the generator, resulting in a high-quality CT volume both visually and quantitatively. Extensive experiments on a publicly available chest CT dataset demonstrate the effectiveness of the proposed method. It could be a nice enhancement of a low-cost X-ray machine to provide physicians a CT-like 3D volume in several niche applications.</details> | ‚ùå
| 62 |[CoreDiff](https://arxiv.org/pdf/2304.01814)|CoreDiff: Contextual Error-Modulated Generalized Diffusion Model for Low-Dose CT Denoising and Generalization|IEEE TMI(2023)|CT|Point|a diffusion-based model using a mean-preserving degradation operator and CLEAR-Net for low-dose CT denoising, enhancing image quality with fewer sampling steps|<details><summary>Click</summary>Low-dose computed tomography (CT) images suffer from noise and artifacts due to photon starvation and electronic noise. Recently, some works have attempted to use diffusion models to address the over-smoothness and training instability encountered by previous deep-learning-based denoising models. However, diffusion models suffer from long inference time due to a large number of sampling steps involved. Very recently, cold diffusion model generalizes classical diffusion models and has greater flexibility. Inspired by cold diffusion, this paper presents a novel COntextual eRror-modulated gEneralized Diffusion model for low-dose CT (LDCT) denoising, termed CoreDiff. First, CoreDiff utilizes LDCT images to displace the random Gaussian noise and employs a novel mean-preserving degradation operator to mimic the physical process of CT degradation, significantly reducing sampling steps thanks to the informative LDCT images as the starting point of the sampling process. Second, to alleviate the error accumulation problem caused by the imperfect restoration operator in the sampling process, we propose a novel ContextuaL Error-modulAted Restoration Network (CLEAR-Net), which can leverage contextual information to constrain the sampling process from structural distortion and modulate time step embedding features for better alignment with the input at the next time step. Third, to rapidly generalize the trained model to a new, unseen dose level with as few resources as possible, we devise a one-shot learning framework to make CoreDiff generalize faster and better using only one single LDCT image (un)paired with normal-dose CT (NDCT). Extensive experimental results on four datasets demonstrate that our CoreDiff outperforms competing methods in denoising and generalization performance, with clinically acceptable inference time. Source code is made available at https://github.com/qgao21/CoreDiff.</details> | [üîó](https://github.com/qgao21/CoreDiff)
| 63 |[Liu et al.](https://www.pure.ed.ac.uk/ws/portalfiles/portal/482363230/LiuEtalIEEETMI2024Geometry-awareAttenuationLearning.pdf)|Geometry-Aware Attenuation Learning for Sparse-View CBCT Reconstruction|TMI(2025)|CT|IPE|A geometry-aware encoder-decoder framework for sparseview CBCT reconstruction by  leveraging the prior knowledge and back-projecting multiview 2D features into 3D space|<details><summary>Click</summary>Cone Beam Computed Tomography (CBCT) plays a vital role in clinical imaging. Traditional methods typically require hundreds of 2D X-ray projections to reconstruct a high-quality 3D CBCT image, leading to considerable radiation exposure. This has led to a growing interest in sparse-view CBCT reconstruction to reduce radiation doses. While recent advances, including deep learning and neural rendering algorithms, have made strides in this area, these methods either produce unsatisfactory results or suffer from time inefficiency of individual optimization. In this paper, we introduce a novel geometry-aware encoder-decoder framework to solve this problem. Our framework starts by encoding multi-view 2D features from various 2D X-ray projections with a 2D CNN encoder. Leveraging the geometry of CBCT scanning, it then back-projects the multi-view 2D features into the 3D space to formulate a comprehensive volumetric feature map, followed by a 3D CNN decoder to recover 3D CBCT image. Importantly, our approach respects the geometric relationship between 3D CBCT image and its 2D X-ray projections during feature back projection stage, and enjoys the prior knowledge learned from the data population. This ensures its adaptability in dealing with extremely sparse view inputs without individual training, such as scenarios with only 5 or 10 X-ray projections. Extensive evaluations on two simulated datasets and one real-world dataset demonstrate exceptional reconstruction quality and time efficiency of our method.</details> | ‚ùå
